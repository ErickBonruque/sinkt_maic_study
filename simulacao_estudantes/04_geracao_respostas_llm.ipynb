{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Geração de Respostas via LLM\n",
    "\n",
    "Este notebook implementa a **Etapa 4** do pipeline SINKT: enriquecimento das interações com respostas textuais e justificativas de erro geradas por LLM.\n",
    "\n",
    "## Objetivo\n",
    "Gerar respostas realistas e justificativas de erro usando LLM para as interações previamente simuladas.\n",
    "\n",
    "## Entrada\n",
    "- `data/output/notebooks/simulacao_interacoes/interactions_bkt.json`: Interações simuladas (sem respostas LLM)\n",
    "\n",
    "## Saída\n",
    "- `data/output/notebooks/geracao_respostas_llm/interactions_complete.json`: Interações completas com respostas e justificativas LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    print(\"✅ OpenAI importado com sucesso\")\n",
    "except ImportError:\n",
    "    print(\"❌ OpenAI não instalado. Instale com: pip install openai\")\n",
    "    raise\n",
    "\n",
    "print(\"✅ Bibliotecas importadas com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializar Cliente OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "print(\"✅ Cliente OpenAI inicializado\")\n",
    "print(\"   Usando modelo: gpt-4o-mini\")\n",
    "print(\"   API Key: Carregada de OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/output/notebooks/simulacao_interacoes/interactions_bkt.json', 'r', encoding='utf-8') as f:\n",
    "    interactions_data = json.load(f)\n",
    "interactions = interactions_data['interactions']\n",
    "\n",
    "with open('data/json/questions_graph.json', 'r', encoding='utf-8') as f:\n",
    "    questions_data = json.load(f)\n",
    "questions = {q['id']: q for q in questions_data.get('questions', [])}\n",
    "\n",
    "print(f\"✅ Dados carregados:\")\n",
    "print(f\"  - Interações: {len(interactions)}\")\n",
    "print(f\"  - Questões: {len(questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "LLM_TEMPERATURE = 0.7\n",
    "LLM_MAX_TOKENS = 200\n",
    "LLM_TIMEOUT = 10\n",
    "\n",
    "CHECKPOINT_INTERVAL = 100\n",
    "CHECKPOINT_FILE = 'data/output/notebooks/geracao_respostas_llm/interactions_llm_checkpoint.json'\n",
    "\n",
    "print(f\"✅ Configurações:\")\n",
    "print(f\"  - Modelo LLM: {LLM_MODEL}\")\n",
    "print(f\"  - Temperatura: {LLM_TEMPERATURE}\")\n",
    "print(f\"  - Checkpoint a cada: {CHECKPOINT_INTERVAL} interações\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "LLM_TEMPERATURE = 0.7\n",
    "LLM_MAX_TOKENS = 200\n",
    "LLM_TIMEOUT = 10\n",
    "\n",
    "CHECKPOINT_INTERVAL = 100\n",
    "CHECKPOINT_FILE = 'data/output/interactions_llm_checkpoint.json'\n",
    "\n",
    "print(f\"✅ Configurações:\")\n",
    "print(f\"  - Modelo LLM: {LLM_MODEL}\")\n",
    "print(f\"  - Temperatura: {LLM_TEMPERATURE}\")\n",
    "print(f\"  - Checkpoint a cada: {CHECKPOINT_INTERVAL} interações\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Geração com LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_llm(question: Dict, is_correct: bool, error_type: str = None) -> str:\n",
    "    \"\"\"Gera resposta realista usando OpenAI LLM.\"\"\"\n",
    "    question_text = question.get('q', 'Questão desconhecida')\n",
    "    question_type = question.get('type', 'descriptive')\n",
    "    \n",
    "    if question_type == 'multiple_choice':\n",
    "        if is_correct:\n",
    "            correct_answer = question.get('ans', 'A')\n",
    "            return f\"Opção {correct_answer}\"\n",
    "        else:\n",
    "            options = ['A', 'B', 'C', 'D']\n",
    "            correct = question.get('ans', 'A')\n",
    "            wrong_options = [o for o in options if o != correct]\n",
    "            return f\"Opção {random.choice(wrong_options)}\"\n",
    "    \n",
    "    try:\n",
    "        if is_correct:\n",
    "            prompt = f\"\"\"Você é um estudante de Linux que entendeu bem o conceito. \n",
    "            Responda de forma clara e concisa (2-3 frases) a seguinte questão:\n",
    "            \n",
    "            {question_text}\n",
    "            \n",
    "            Responda como um estudante competente, sem ser muito formal.\"\"\"\n",
    "        else:\n",
    "            error_descriptions = {\n",
    "                'misconception': 'confundindo conceitos similares',\n",
    "                'careless': 'cometendo um erro por descuido',\n",
    "                'slip': 'cometendo um erro de digitação ou distração',\n",
    "                'incomplete': 'dando uma resposta incompleta',\n",
    "                'misunderstanding': 'entendendo mal o enunciado'\n",
    "            }\n",
    "            error_desc = error_descriptions.get(error_type, 'cometendo um erro')\n",
    "            \n",
    "            prompt = f\"\"\"Você é um estudante de Linux que tem dificuldades.\n",
    "            Responda de forma realista (2-3 frases) a seguinte questão, {error_desc}:\n",
    "            \n",
    "            {question_text}\n",
    "            \n",
    "            Responda como um estudante que não entendeu completamente.\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Você é um estudante respondendo questões sobre Linux.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=LLM_TEMPERATURE,\n",
    "            max_tokens=LLM_MAX_TOKENS,\n",
    "            timeout=LLM_TIMEOUT\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Erro ao chamar LLM: {str(e)}\")\n",
    "        return \"[Resposta indisponível - LLM erro]\"\n",
    "\n",
    "def generate_error_justification_with_llm(question: Dict, student_response: str, \n",
    "                                         error_type: str, concept_name: str) -> str:\n",
    "    \"\"\"Gera justificativa de erro usando LLM para análise mais profunda.\"\"\"\n",
    "    question_text = question.get('q', 'Questão desconhecida')\n",
    "    correct_answer = question.get('ans', 'N/A')\n",
    "    \n",
    "    try:\n",
    "        prompt = f\"\"\"Analise o erro do estudante de forma concisa (1-2 frases).\n",
    "\n",
    "Questão: {question_text}\n",
    "Resposta Correta: {correct_answer}\n",
    "Resposta do Estudante: {student_response}\n",
    "Tipo de Erro: {error_type}\n",
    "Conceito: {concept_name}\n",
    "\n",
    "Forneça uma justificativa educacional breve sobre o erro cometido.\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Você é um tutor analisando erros de estudantes.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            max_tokens=150,\n",
    "            timeout=LLM_TIMEOUT\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Erro do tipo {error_type} relacionado ao conceito '{concept_name}'.\"\n",
    "\n",
    "print(\"✅ Funções de geração LLM definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_file = 'data/output/notebooks/geracao_respostas_llm/interactions_complete.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(enriched_interactions: List[Dict], last_idx: int):\n",
    "    \"\"\"Salva checkpoint incremental.\"\"\"\n",
    "    checkpoint_data = {\n",
    "        \"checkpoint_info\": {\n",
    "            \"last_interaction_processed\": last_idx,\n",
    "            \"total_processed\": len(enriched_interactions),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"interactions\": enriched_interactions\n",
    "    }\n",
    "    \n",
    "    with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"  Checkpoint salvo: {len(enriched_interactions)} interações processadas\")\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Carrega checkpoint se existir.\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data['interactions'], data['checkpoint_info']['last_interaction_processed']\n",
    "    return [], -1\n",
    "\n",
    "def enrich_interactions_with_llm(interactions: List[Dict], questions: Dict, \n",
    "                                resume: bool = True) -> List[Dict]:\n",
    "    \"\"\"Enriquece interações com respostas e justificativas LLM.\"\"\"\n",
    "    enriched_interactions = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    if resume:\n",
    "        enriched_interactions, last_processed = load_checkpoint()\n",
    "        if last_processed >= 0:\n",
    "            start_idx = last_processed + 1\n",
    "            print(f\"Checkpoint encontrado! Retomando da interação {start_idx + 1}\")\n",
    "            print(f\"   Interações já processadas: {len(enriched_interactions)}\")\n",
    "    \n",
    "    total_interactions = len(interactions)\n",
    "    \n",
    "    for idx in range(start_idx, total_interactions):\n",
    "        interaction = interactions[idx].copy()\n",
    "        \n",
    "        if (idx + 1) % 100 == 0 or idx == start_idx:\n",
    "            print(f\"  Processando interação {idx + 1}/{total_interactions}...\")\n",
    "        \n",
    "        question_id = interaction['question_id']\n",
    "        question = questions.get(question_id, {})\n",
    "        \n",
    "        is_correct = interaction['is_correct']\n",
    "        error_type = interaction.get('error_type')\n",
    "        \n",
    "        response = generate_response_with_llm(question, is_correct, error_type)\n",
    "        interaction['response'] = response\n",
    "        \n",
    "        if not is_correct and error_type:\n",
    "            concept_name = interaction.get('concept_name', 'Conceito')\n",
    "            justification = generate_error_justification_with_llm(\n",
    "                question, response, error_type, concept_name\n",
    "            )\n",
    "            interaction['error_justification'] = justification\n",
    "        else:\n",
    "            interaction['error_justification'] = None\n",
    "        \n",
    "        enriched_interactions.append(interaction)\n",
    "        \n",
    "        if (idx + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "            save_checkpoint(enriched_interactions, idx)\n",
    "    \n",
    "    save_checkpoint(enriched_interactions, total_interactions - 1)\n",
    "    \n",
    "    return enriched_interactions\n",
    "\n",
    "print(\"Iniciando enriquecimento com LLM...\")\n",
    "print(f\"   Total de interações: {len(interactions)}\")\n",
    "print()\n",
    "\n",
    "enriched_interactions = enrich_interactions_with_llm(interactions, questions, resume=True)\n",
    "\n",
    "print(f\"\\n✅ {len(enriched_interactions)} interações enriquecidas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvamento das Interações Completas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {\n",
    "    \"metadata\": {\n",
    "        \"description\": \"Conjunto completo de interações com respostas e justificativas LLM\",\n",
    "        \"version\": \"4.0.0\",\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"llm_model\": LLM_MODEL,\n",
    "        \"llm_temperature\": LLM_TEMPERATURE,\n",
    "        \"total_interactions\": len(enriched_interactions)\n",
    "    },\n",
    "    \"interactions\": enriched_interactions\n",
    "}\n",
    "\n",
    "output_file = 'data/output/notebooks/geracao_respostas_llm/interactions_complete.json'\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    os.remove(CHECKPOINT_FILE)\n",
    "    print(f\"Checkpoint removido (processamento completo)\")\n",
    "\n",
    "print(f\"\\n✅ Interações completas salvas em: {output_file}\")\n",
    "print(f\"Total de interações: {len(enriched_interactions)}\")\n",
    "print(f\"Tamanho do arquivo: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualidade das Respostas LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_llm_response_quality(interactions: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Analisa qualidade das respostas geradas por LLM.\"\"\"\n",
    "    total_responses = len(interactions)\n",
    "    \n",
    "    empty_responses = sum(1 for i in interactions if not i.get('response') or len(i.get('response', '').strip()) == 0)\n",
    "    valid_responses = total_responses - empty_responses\n",
    "    \n",
    "    response_lengths = [len(i.get('response', '')) for i in interactions if i.get('response')]\n",
    "    \n",
    "    incorrect_with_justification = sum(\n",
    "        1 for i in interactions \n",
    "        if not i['is_correct'] and i.get('error_justification')\n",
    "    )\n",
    "    \n",
    "    total_incorrect = sum(1 for i in interactions if not i['is_correct'])\n",
    "    \n",
    "    return {\n",
    "        'total_responses': total_responses,\n",
    "        'valid_responses': valid_responses,\n",
    "        'empty_responses': empty_responses,\n",
    "        'validity_percentage': (valid_responses / total_responses * 100) if total_responses > 0 else 0,\n",
    "        'response_length_stats': {\n",
    "            'mean': np.mean(response_lengths) if response_lengths else 0,\n",
    "            'median': np.median(response_lengths) if response_lengths else 0,\n",
    "            'min': np.min(response_lengths) if response_lengths else 0,\n",
    "            'max': np.max(response_lengths) if response_lengths else 0\n",
    "        },\n",
    "        'error_justifications': {\n",
    "            'total_errors': total_incorrect,\n",
    "            'with_justification': incorrect_with_justification,\n",
    "            'coverage_percentage': (incorrect_with_justification / total_incorrect * 100) if total_incorrect > 0 else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "llm_quality = analyze_llm_response_quality(enriched_interactions)\n",
    "\n",
    "print(\"\\n✅ Qualidade das Respostas LLM:\\n\")\n",
    "print(f\"  Total de Respostas: {llm_quality['total_responses']}\")\n",
    "print(f\"  Respostas Válidas: {llm_quality['valid_responses']} ({llm_quality['validity_percentage']:.1f}%)\")\n",
    "print(f\"  Respostas Vazias: {llm_quality['empty_responses']}\")\n",
    "print(f\"\\n  Tamanho das Respostas (caracteres):\")\n",
    "print(f\"    Média: {llm_quality['response_length_stats']['mean']:.0f}\")\n",
    "print(f\"    Mediana: {llm_quality['response_length_stats']['median']:.0f}\")\n",
    "print(f\"    Range: [{llm_quality['response_length_stats']['min']:.0f}, {llm_quality['response_length_stats']['max']:.0f}]\")\n",
    "print(f\"\\n  Justificativas de Erro:\")\n",
    "print(f\"    Total de Erros: {llm_quality['error_justifications']['total_errors']}\")\n",
    "print(f\"    Com Justificativa: {llm_quality['error_justifications']['with_justification']} ({llm_quality['error_justifications']['coverage_percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplos de Respostas e Justificativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n✅ Exemplos de Interações Completas:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "correct_examples = [i for i in enriched_interactions if i['is_correct']][:2]\n",
    "incorrect_examples = [i for i in enriched_interactions if not i['is_correct']][:2]\n",
    "\n",
    "print(\"\\nEXEMPLOS DE RESPOSTAS CORRETAS:\\n\")\n",
    "for i, interaction in enumerate(correct_examples, 1):\n",
    "    print(f\"Exemplo {i}:\")\n",
    "    print(f\"  ID: {interaction['interaction_id']}\")\n",
    "    print(f\"  Estudante: {interaction['student_id']}\")\n",
    "    print(f\"  Conceito: {interaction.get('concept_name', 'N/A')}\")\n",
    "    print(f\"  Resposta: {interaction['response'][:150]}...\" if len(interaction['response']) > 150 else f\"  Resposta: {interaction['response']}\")\n",
    "    print(f\"  Domínio: {interaction['mastery_before']:.3f} -> {interaction['mastery_after']:.3f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nEXEMPLOS DE RESPOSTAS INCORRETAS COM JUSTIFICATIVA:\\n\")\n",
    "for i, interaction in enumerate(incorrect_examples, 1):\n",
    "    print(f\"Exemplo {i}:\")\n",
    "    print(f\"  ID: {interaction['interaction_id']}\")\n",
    "    print(f\"  Estudante: {interaction['student_id']}\")\n",
    "    print(f\"  Conceito: {interaction.get('concept_name', 'N/A')}\")\n",
    "    print(f\"  Tipo de Erro: {interaction.get('error_type', 'N/A')}\")\n",
    "    print(f\"  Resposta: {interaction['response'][:150]}...\" if len(interaction['response']) > 150 else f\"  Resposta: {interaction['response']}\")\n",
    "    print(f\"  Justificativa: {interaction.get('error_justification', 'N/A')}\")\n",
    "    print(f\"  Domínio: {interaction['mastery_before']:.3f} -> {interaction['mastery_after']:.3f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Insights das Justificativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_justification_insights(interactions: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Analisa insights das justificativas de erro geradas por LLM.\"\"\"\n",
    "    error_justifications = [\n",
    "        {\n",
    "            'error_type': i['error_type'],\n",
    "            'concept': i.get('concept_name', 'N/A'),\n",
    "            'justification': i.get('error_justification', ''),\n",
    "            'justification_length': len(i.get('error_justification', ''))\n",
    "        }\n",
    "        for i in interactions if not i['is_correct'] and i.get('error_justification')\n",
    "    ]\n",
    "    \n",
    "    justification_lengths = [j['justification_length'] for j in error_justifications]\n",
    "    \n",
    "    insights_by_error_type = defaultdict(list)\n",
    "    for j in error_justifications:\n",
    "        insights_by_error_type[j['error_type']].append(j['justification_length'])\n",
    "    \n",
    "    error_type_stats = {}\n",
    "    for error_type, lengths in insights_by_error_type.items():\n",
    "        error_type_stats[error_type] = {\n",
    "            'count': len(lengths),\n",
    "            'avg_length': np.mean(lengths) if lengths else 0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'total_justifications': len(error_justifications),\n",
    "        'justification_length_stats': {\n",
    "            'mean': np.mean(justification_lengths) if justification_lengths else 0,\n",
    "            'median': np.median(justification_lengths) if justification_lengths else 0,\n",
    "            'min': np.min(justification_lengths) if justification_lengths else 0,\n",
    "            'max': np.max(justification_lengths) if justification_lengths else 0\n",
    "        },\n",
    "        'by_error_type': error_type_stats\n",
    "    }\n",
    "\n",
    "justification_insights = analyze_error_justification_insights(enriched_interactions)\n",
    "\n",
    "print(\"\\n✅ Análise de Insights das Justificativas:\\n\")\n",
    "print(f\"  Total de Justificativas: {justification_insights['total_justifications']}\")\n",
    "print(f\"\\n  Tamanho das Justificativas (caracteres):\")\n",
    "print(f\"    Média: {justification_insights['justification_length_stats']['mean']:.0f}\")\n",
    "print(f\"    Mediana: {justification_insights['justification_length_stats']['median']:.0f}\")\n",
    "print(f\"    Range: [{justification_insights['justification_length_stats']['min']:.0f}, {justification_insights['justification_length_stats']['max']:.0f}]\")\n",
    "print(f\"\\n  Justificativas por Tipo de Erro:\")\n",
    "for error_type, stats in justification_insights['by_error_type'].items():\n",
    "    print(f\"    - {error_type}: {stats['count']} justificativas (média {stats['avg_length']:.0f} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo da Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ GERAÇÃO DE RESPOSTAS VIA LLM CONCLUÍDA COM SUCESSO!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n  Arquivo gerado: {output_file}\")\n",
    "print(f\"\\n  Resumo:\")\n",
    "print(f\"    - Total de interações: {len(enriched_interactions)}\")\n",
    "print(f\"    - Respostas válidas: {llm_quality['validity_percentage']:.1f}%\")\n",
    "print(f\"    - Justificativas de erro: {llm_quality['error_justifications']['coverage_percentage']:.1f}%\")\n",
    "print(f\"    - Modelo LLM: {LLM_MODEL}\")\n",
    "print(f\"\\n✅ Pipeline de simulação completo!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
