{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Geração de Respostas via LLM\n",
        "\n",
        "Este notebook implementa a **Etapa 4** do pipeline SINKT: geração de respostas simuladas usando a API da OpenAI.\n",
        "\n",
        "## Objetivo\n",
        "Enriquecer as interações BKT com:\n",
        "- Respostas textuais simuladas (para questões descritivas e múltipla escolha)\n",
        "- Justificativas de erro baseadas no tipo de erro e resposta do aluno\n",
        "\n",
        "## Entrada\n",
        "- `data/output/notebooks/simulacao_interacoes/interactions_bkt.json`\n",
        "\n",
        "## Saída\n",
        "- `data/output/notebooks/geracao_respostas_llm/interactions_complete.json`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importação de Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bibliotecas importadas com sucesso\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuração da API OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API configurada\n",
            "  - Modelo: gpt-4.1-mini\n",
            "  - Temperature: 0.7\n",
            "  - Max Tokens: 300\n"
          ]
        }
      ],
      "source": [
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not OPENAI_API_KEY or OPENAI_API_KEY == \"SUA_CHAVE_OPENAI_AQUI\":\n",
        "    raise ValueError(\"OPENAI_API_KEY nao configurada no arquivo .env\")\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "MODEL_NAME = \"gpt-4.1-mini\"\n",
        "TEMPERATURE = 0.7\n",
        "MAX_TOKENS = 300\n",
        "\n",
        "print(f\"OpenAI API configurada\")\n",
        "print(f\"  - Modelo: {MODEL_NAME}\")\n",
        "print(f\"  - Temperature: {TEMPERATURE}\")\n",
        "print(f\"  - Max Tokens: {MAX_TOKENS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carregamento de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dados carregados:\n",
            "  - Interacoes BKT: 4499\n",
            "  - Questoes: 680\n",
            "  - Conceitos: 251\n",
            "  - Estudantes: 100\n"
          ]
        }
      ],
      "source": [
        "INPUT_FILE = 'data/output/notebooks/simulacao_interacoes/interactions_bkt.json'\n",
        "OUTPUT_DIR = 'data/output/notebooks/geracao_respostas_llm'\n",
        "OUTPUT_FILE = os.path.join(OUTPUT_DIR, 'interactions_complete.json')\n",
        "CHECKPOINT_FILE = os.path.join(OUTPUT_DIR, 'checkpoint.json')\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    bkt_data = json.load(f)\n",
        "interactions_bkt = bkt_data['interactions']\n",
        "\n",
        "with open('data/json/questions_graph.json', 'r', encoding='utf-8') as f:\n",
        "    questions_data = json.load(f)\n",
        "questions_list = questions_data.get('questions', [])\n",
        "questions_map = {q['id']: q for q in questions_list}\n",
        "\n",
        "with open('data/json/concepts_graph.json', 'r', encoding='utf-8') as f:\n",
        "    concepts_data = json.load(f)\n",
        "concepts_list = concepts_data.get('concepts', [])\n",
        "concepts_map = {c['id']: c for c in concepts_list}\n",
        "\n",
        "with open('data/output/notebooks/geracao_estudantes/students.json', 'r', encoding='utf-8') as f:\n",
        "    students_data = json.load(f)\n",
        "students_list = students_data['students']\n",
        "students_map = {s['id']: s for s in students_list}\n",
        "\n",
        "print(f\"Dados carregados:\")\n",
        "print(f\"  - Interacoes BKT: {len(interactions_bkt)}\")\n",
        "print(f\"  - Questoes: {len(questions_map)}\")\n",
        "print(f\"  - Conceitos: {len(concepts_map)}\")\n",
        "print(f\"  - Estudantes: {len(students_map)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuração de Parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuracoes:\n",
            "  - Checkpoint a cada: 50 interacoes\n",
            "  - Rate limit delay: 0.1s\n",
            "  - Seed: 42\n"
          ]
        }
      ],
      "source": [
        "CHECKPOINT_INTERVAL = 50\n",
        "RATE_LIMIT_DELAY = 0.1\n",
        "SEED = 42\n",
        "\n",
        "ERROR_TYPE_DESCRIPTIONS = {\n",
        "    'misconception': {\n",
        "        'description': 'Confusao conceitual - estudante confundiu com conceito similar',\n",
        "        'prompt_hint': 'Demonstre confusao entre conceitos similares, misturando definicoes ou aplicacoes'\n",
        "    },\n",
        "    'careless': {\n",
        "        'description': 'Erro por descuido - estudante conhece mas nao prestou atencao',\n",
        "        'prompt_hint': 'Responda de forma apressada, cometendo erros bobos que demonstram falta de atencao'\n",
        "    },\n",
        "    'slip': {\n",
        "        'description': 'Erro por distracao - estudante sabe mas cometeu deslize',\n",
        "        'prompt_hint': 'Mostre conhecimento parcial mas cometa um erro de digitacao ou troca de termos'\n",
        "    },\n",
        "    'incomplete': {\n",
        "        'description': 'Resposta incompleta - faltam elementos importantes',\n",
        "        'prompt_hint': 'Responda apenas parcialmente, omitindo partes importantes da explicacao'\n",
        "    },\n",
        "    'misunderstanding': {\n",
        "        'description': 'Mal entendimento do enunciado',\n",
        "        'prompt_hint': 'Responda como se tivesse entendido a pergunta de forma diferente'\n",
        "    }\n",
        "}\n",
        "\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "print(f\"Configuracoes:\")\n",
        "print(f\"  - Checkpoint a cada: {CHECKPOINT_INTERVAL} interacoes\")\n",
        "print(f\"  - Rate limit delay: {RATE_LIMIT_DELAY}s\")\n",
        "print(f\"  - Seed: {SEED}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funções de Geração de Respostas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Funcoes de geracao de prompt definidas\n"
          ]
        }
      ],
      "source": [
        "def get_student_context(student_id: str) -> str:\n",
        "    \"\"\"Retorna contexto do estudante para o prompt.\"\"\"\n",
        "    student = students_map.get(student_id, {})\n",
        "    profile_id = student.get('profile_id', 'balanced')\n",
        "    \n",
        "    profile_descriptions = {\n",
        "        'quick_learner': 'aprende rapido, confiante, as vezes superficial',\n",
        "        'careful': 'cauteloso, detalhista, prefere ter certeza',\n",
        "        'struggling': 'tem dificuldades, inseguro, precisa de mais tempo',\n",
        "        'intuitive': 'intuitivo, criativo, pode pular etapas',\n",
        "        'logical': 'logico, metodico, segue procedimentos',\n",
        "        'balanced': 'equilibrado, consistente, desempenho medio'\n",
        "    }\n",
        "    \n",
        "    return profile_descriptions.get(profile_id, 'estudante padrao')\n",
        "\n",
        "\n",
        "def generate_response_prompt(question: Dict, is_correct: bool, \n",
        "                            error_type: Optional[str], student_context: str) -> str:\n",
        "    \"\"\"Gera o prompt para a API baseado no contexto.\"\"\"\n",
        "    q_text = question.get('q', '')\n",
        "    q_type = question.get('type', 'descriptive')\n",
        "    concept_name = question.get('c_name', '')\n",
        "    explanation = question.get('exp', '')\n",
        "    \n",
        "    base_prompt = f\"\"\"Voce esta simulando a resposta de um estudante de Linux/Shell Script.\n",
        "Contexto do estudante: {student_context}\n",
        "Conceito avaliado: {concept_name}\n",
        "Pergunta: {q_text}\n",
        "\n",
        "Informacao correta (para referencia): {explanation}\n",
        "\"\"\"\n",
        "    \n",
        "    if q_type == 'multiple_choice':\n",
        "        options = question.get('opt', {})\n",
        "        correct_answer = question.get('ans', 'A')\n",
        "        options_text = '\\n'.join([f\"{k}: {v}\" for k, v in options.items()])\n",
        "        \n",
        "        if is_correct:\n",
        "            return f\"\"\"{base_prompt}\n",
        "Opcoes:\n",
        "{options_text}\n",
        "\n",
        "O estudante ACERTOU. Responda APENAS com \"Opcao {correct_answer}\" (nada mais).\"\"\"\n",
        "        else:\n",
        "            error_hint = ERROR_TYPE_DESCRIPTIONS.get(error_type, {}).get('prompt_hint', '')\n",
        "            wrong_options = [k for k in options.keys() if k != correct_answer]\n",
        "            wrong_choice = random.choice(wrong_options) if wrong_options else 'A'\n",
        "            return f\"\"\"{base_prompt}\n",
        "Opcoes:\n",
        "{options_text}\n",
        "\n",
        "O estudante ERROU (tipo: {error_type}). {error_hint}\n",
        "Responda APENAS com \"Opcao {wrong_choice}\" (nada mais).\"\"\"\n",
        "    \n",
        "    else:  # descriptive\n",
        "        if is_correct:\n",
        "            return f\"\"\"{base_prompt}\n",
        "O estudante ACERTOU a questao.\n",
        "Gere uma resposta CORRETA e completa de 2-4 frases.\n",
        "Escreva em linguagem natural de estudante, nao muito formal.\"\"\"\n",
        "        else:\n",
        "            error_hint = ERROR_TYPE_DESCRIPTIONS.get(error_type, {}).get('prompt_hint', '')\n",
        "            return f\"\"\"{base_prompt}\n",
        "O estudante ERROU a questao (tipo de erro: {error_type}).\n",
        "Instrucao especifica: {error_hint}\n",
        "Gere uma resposta INCORRETA de 2-4 frases que demonstre esse tipo de erro.\n",
        "Escreva em linguagem natural de estudante.\"\"\"\n",
        "\n",
        "\n",
        "def generate_error_explanation_prompt(question: Dict, error_type: str, \n",
        "                                      response: str, concept_name: str) -> str:\n",
        "    \"\"\"Gera prompt para justificativa do erro.\"\"\"\n",
        "    q_text = question.get('q', '')\n",
        "    correct_info = question.get('exp', '')\n",
        "    error_desc = ERROR_TYPE_DESCRIPTIONS.get(error_type, {}).get('description', 'Erro generico')\n",
        "    \n",
        "    return f\"\"\"Analise o erro do estudante e gere uma justificativa pedagogica.\n",
        "\n",
        "Conceito: {concept_name}\n",
        "Pergunta: {q_text}\n",
        "Resposta do estudante: {response}\n",
        "Tipo de erro: {error_type} ({error_desc})\n",
        "Informacao correta: {correct_info}\n",
        "\n",
        "Gere uma justificativa de 1-2 frases explicando:\n",
        "1. O que o estudante errou especificamente\n",
        "2. Qual conceito precisa ser reforçado\n",
        "\n",
        "Seja objetivo e tecnico. Use terceira pessoa (\"O estudante...\").\"\"\"\n",
        "\n",
        "\n",
        "print(\"Funcoes de geracao de prompt definidas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Funcoes de chamada da API definidas\n"
          ]
        }
      ],
      "source": [
        "def call_openai_api(prompt: str, max_retries: int = 3) -> Optional[str]:\n",
        "    \"\"\"Chama a API da OpenAI com retry logic.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Voce simula respostas de estudantes de forma realista.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=TEMPERATURE,\n",
        "                max_tokens=MAX_TOKENS\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = (attempt + 1) * 2\n",
        "                print(f\"  Erro na API (tentativa {attempt + 1}): {str(e)[:50]}. Aguardando {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"  Erro apos {max_retries} tentativas: {str(e)[:100]}\")\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def generate_student_response(interaction: Dict, question: Dict) -> Tuple[str, Optional[str]]:\n",
        "    \"\"\"Gera resposta do estudante e justificativa de erro se aplicavel.\"\"\"\n",
        "    student_id = interaction['student_id']\n",
        "    is_correct = interaction['is_correct']\n",
        "    error_type = interaction.get('error_type')\n",
        "    concept_name = question.get('c_name', '')\n",
        "    \n",
        "    student_context = get_student_context(student_id)\n",
        "    \n",
        "    # Gera resposta\n",
        "    response_prompt = generate_response_prompt(question, is_correct, error_type, student_context)\n",
        "    response = call_openai_api(response_prompt)\n",
        "    \n",
        "    if not response:\n",
        "        response = \"Nao sei responder essa pergunta.\" if not is_correct else \"Resposta correta gerada.\"\n",
        "    \n",
        "    # Gera justificativa de erro se aplicavel\n",
        "    error_explanation = None\n",
        "    if not is_correct and error_type:\n",
        "        explanation_prompt = generate_error_explanation_prompt(\n",
        "            question, error_type, response, concept_name\n",
        "        )\n",
        "        error_explanation = call_openai_api(explanation_prompt)\n",
        "        \n",
        "        if not error_explanation:\n",
        "            error_explanation = f\"Estudante cometeu erro do tipo '{error_type}'. Necessario reforco em '{concept_name}'.\"\n",
        "    \n",
        "    return response, error_explanation\n",
        "\n",
        "\n",
        "print(\"Funcoes de chamada da API definidas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sistema de Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sistema de checkpoint configurado\n"
          ]
        }
      ],
      "source": [
        "def save_checkpoint(processed_interactions: List[Dict], processed_count: int):\n",
        "    \"\"\"Salva checkpoint do progresso.\"\"\"\n",
        "    checkpoint_data = {\n",
        "        'processed_count': processed_count,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'interactions': processed_interactions\n",
        "    }\n",
        "    with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:\n",
        "        json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "\n",
        "def load_checkpoint() -> Tuple[List[Dict], int]:\n",
        "    \"\"\"Carrega checkpoint se existir.\"\"\"\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:\n",
        "            checkpoint_data = json.load(f)\n",
        "        return checkpoint_data['interactions'], checkpoint_data['processed_count']\n",
        "    return [], 0\n",
        "\n",
        "\n",
        "def remove_checkpoint():\n",
        "    \"\"\"Remove arquivo de checkpoint.\"\"\"\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        os.remove(CHECKPOINT_FILE)\n",
        "        print(\"Checkpoint removido (processamento completo)\")\n",
        "\n",
        "\n",
        "print(\"Sistema de checkpoint configurado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processamento das Interações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando processamento de 4499 interacoes...\n",
            "Checkpoint: 50/4499 (1.1%) - ETA: 192.8min\n",
            "Checkpoint: 100/4499 (2.2%) - ETA: 165.3min\n",
            "Checkpoint: 150/4499 (3.3%) - ETA: 163.8min\n",
            "Checkpoint: 200/4499 (4.4%) - ETA: 151.8min\n",
            "Checkpoint: 250/4499 (5.6%) - ETA: 147.1min\n",
            "Checkpoint: 300/4499 (6.7%) - ETA: 145.9min\n",
            "Checkpoint: 350/4499 (7.8%) - ETA: 145.9min\n",
            "Checkpoint: 400/4499 (8.9%) - ETA: 137.6min\n",
            "Checkpoint: 450/4499 (10.0%) - ETA: 138.3min\n",
            "Checkpoint: 500/4499 (11.1%) - ETA: 138.7min\n",
            "Checkpoint: 550/4499 (12.2%) - ETA: 142.1min\n",
            "Checkpoint: 600/4499 (13.3%) - ETA: 139.9min\n",
            "Checkpoint: 650/4499 (14.4%) - ETA: 141.2min\n",
            "Checkpoint: 700/4499 (15.6%) - ETA: 140.3min\n",
            "Checkpoint: 750/4499 (16.7%) - ETA: 137.0min\n",
            "Checkpoint: 800/4499 (17.8%) - ETA: 133.9min\n",
            "Checkpoint: 850/4499 (18.9%) - ETA: 131.5min\n",
            "Checkpoint: 900/4499 (20.0%) - ETA: 128.4min\n",
            "Checkpoint: 950/4499 (21.1%) - ETA: 126.2min\n",
            "Checkpoint: 1000/4499 (22.2%) - ETA: 124.7min\n",
            "Checkpoint: 1050/4499 (23.3%) - ETA: 122.7min\n",
            "Checkpoint: 1100/4499 (24.4%) - ETA: 119.3min\n",
            "Checkpoint: 1150/4499 (25.6%) - ETA: 116.8min\n",
            "Checkpoint: 1200/4499 (26.7%) - ETA: 114.7min\n",
            "Checkpoint: 1250/4499 (27.8%) - ETA: 112.5min\n",
            "Checkpoint: 1300/4499 (28.9%) - ETA: 110.7min\n",
            "Checkpoint: 1350/4499 (30.0%) - ETA: 109.1min\n",
            "Checkpoint: 1400/4499 (31.1%) - ETA: 106.2min\n",
            "Checkpoint: 1450/4499 (32.2%) - ETA: 104.4min\n",
            "Checkpoint: 1500/4499 (33.3%) - ETA: 102.9min\n",
            "Checkpoint: 1550/4499 (34.5%) - ETA: 101.1min\n",
            "Checkpoint: 1600/4499 (35.6%) - ETA: 98.7min\n",
            "Checkpoint: 1650/4499 (36.7%) - ETA: 96.6min\n",
            "Checkpoint: 1700/4499 (37.8%) - ETA: 94.9min\n",
            "Checkpoint: 1750/4499 (38.9%) - ETA: 93.0min\n",
            "Checkpoint: 1800/4499 (40.0%) - ETA: 91.0min\n",
            "Checkpoint: 1850/4499 (41.1%) - ETA: 89.2min\n",
            "Checkpoint: 1900/4499 (42.2%) - ETA: 88.3min\n",
            "Checkpoint: 1950/4499 (43.3%) - ETA: 86.1min\n",
            "Checkpoint: 2000/4499 (44.5%) - ETA: 84.0min\n",
            "Checkpoint: 2050/4499 (45.6%) - ETA: 82.2min\n",
            "Checkpoint: 2100/4499 (46.7%) - ETA: 80.2min\n",
            "Checkpoint: 2150/4499 (47.8%) - ETA: 78.4min\n",
            "Checkpoint: 2200/4499 (48.9%) - ETA: 77.0min\n",
            "Checkpoint: 2250/4499 (50.0%) - ETA: 74.9min\n",
            "Checkpoint: 2300/4499 (51.1%) - ETA: 73.0min\n",
            "Checkpoint: 2350/4499 (52.2%) - ETA: 71.9min\n",
            "Checkpoint: 2400/4499 (53.3%) - ETA: 70.0min\n",
            "Checkpoint: 2450/4499 (54.5%) - ETA: 67.9min\n",
            "Checkpoint: 2500/4499 (55.6%) - ETA: 66.5min\n",
            "Checkpoint: 2550/4499 (56.7%) - ETA: 64.6min\n",
            "Checkpoint: 2600/4499 (57.8%) - ETA: 62.8min\n",
            "Checkpoint: 2650/4499 (58.9%) - ETA: 61.3min\n",
            "Checkpoint: 2700/4499 (60.0%) - ETA: 59.5min\n",
            "Checkpoint: 2750/4499 (61.1%) - ETA: 57.9min\n",
            "Checkpoint: 2800/4499 (62.2%) - ETA: 56.3min\n",
            "Checkpoint: 2850/4499 (63.3%) - ETA: 54.4min\n",
            "Checkpoint: 2900/4499 (64.5%) - ETA: 52.7min\n",
            "Checkpoint: 2950/4499 (65.6%) - ETA: 51.4min\n",
            "Checkpoint: 3000/4499 (66.7%) - ETA: 49.8min\n",
            "Checkpoint: 3050/4499 (67.8%) - ETA: 48.2min\n",
            "Checkpoint: 3100/4499 (68.9%) - ETA: 46.7min\n",
            "Checkpoint: 3150/4499 (70.0%) - ETA: 45.2min\n",
            "Checkpoint: 3200/4499 (71.1%) - ETA: 43.5min\n",
            "Checkpoint: 3250/4499 (72.2%) - ETA: 41.9min\n",
            "Checkpoint: 3300/4499 (73.3%) - ETA: 40.3min\n",
            "Checkpoint: 3350/4499 (74.5%) - ETA: 38.7min\n",
            "Checkpoint: 3400/4499 (75.6%) - ETA: 37.1min\n",
            "Checkpoint: 3450/4499 (76.7%) - ETA: 35.5min\n",
            "Checkpoint: 3500/4499 (77.8%) - ETA: 33.8min\n",
            "Checkpoint: 3550/4499 (78.9%) - ETA: 32.1min\n",
            "Checkpoint: 3600/4499 (80.0%) - ETA: 30.4min\n",
            "Checkpoint: 3650/4499 (81.1%) - ETA: 28.6min\n",
            "Checkpoint: 3700/4499 (82.2%) - ETA: 27.0min\n",
            "Checkpoint: 3750/4499 (83.4%) - ETA: 25.3min\n",
            "Checkpoint: 3800/4499 (84.5%) - ETA: 23.6min\n",
            "Checkpoint: 3850/4499 (85.6%) - ETA: 21.9min\n",
            "Checkpoint: 3900/4499 (86.7%) - ETA: 20.3min\n",
            "Checkpoint: 3950/4499 (87.8%) - ETA: 18.6min\n",
            "Checkpoint: 4000/4499 (88.9%) - ETA: 16.9min\n",
            "Checkpoint: 4050/4499 (90.0%) - ETA: 15.2min\n",
            "Checkpoint: 4100/4499 (91.1%) - ETA: 13.5min\n",
            "Checkpoint: 4150/4499 (92.2%) - ETA: 11.9min\n",
            "Checkpoint: 4200/4499 (93.4%) - ETA: 10.2min\n",
            "Checkpoint: 4250/4499 (94.5%) - ETA: 8.5min\n",
            "Checkpoint: 4300/4499 (95.6%) - ETA: 6.8min\n",
            "Checkpoint: 4350/4499 (96.7%) - ETA: 5.1min\n",
            "Checkpoint: 4400/4499 (97.8%) - ETA: 3.4min\n",
            "Checkpoint: 4450/4499 (98.9%) - ETA: 1.7min\n",
            "\n",
            "Processamento concluido!\n",
            "  - Tempo total: 153.2 min\n",
            "  - Interacoes processadas: 4499\n",
            "  - Taxa: 0.49 int/s\n"
          ]
        }
      ],
      "source": [
        "processed_interactions, start_idx = load_checkpoint()\n",
        "\n",
        "if start_idx > 0:\n",
        "    print(f\"Retomando do checkpoint: {start_idx}/{len(interactions_bkt)} interacoes processadas\")\n",
        "else:\n",
        "    print(f\"Iniciando processamento de {len(interactions_bkt)} interacoes...\")\n",
        "\n",
        "start_time = time.time()\n",
        "error_count = 0\n",
        "success_count = 0\n",
        "\n",
        "for idx in range(start_idx, len(interactions_bkt)):\n",
        "    interaction = interactions_bkt[idx]\n",
        "    question_id = interaction['question_id']\n",
        "    question = questions_map.get(question_id)\n",
        "    \n",
        "    if not question:\n",
        "        print(f\"  Questao nao encontrada: {question_id}\")\n",
        "        continue\n",
        "    \n",
        "    # Gera resposta via LLM\n",
        "    response, error_explanation = generate_student_response(interaction, question)\n",
        "    \n",
        "    # Calcula tempo simulado (baseado em dificuldade e tipo)\n",
        "    base_time = 60 if question.get('type') == 'multiple_choice' else 180\n",
        "    difficulty_mult = {'easy': 0.7, 'medium': 1.0, 'hard': 1.4}.get(question.get('diff', 'medium'), 1.0)\n",
        "    time_spent = int(base_time * difficulty_mult * (0.8 + random.random() * 0.4))\n",
        "    \n",
        "    # Monta interacao completa\n",
        "    complete_interaction = {\n",
        "        'interaction_id': f\"int_{idx:06d}\",\n",
        "        'student_id': interaction['student_id'],\n",
        "        'question_id': question_id,\n",
        "        'concept_id': question.get('c_id', ''),\n",
        "        'question_type': question.get('type', 'descriptive'),\n",
        "        'timestamp': interaction['timestamp'],\n",
        "        'response': response,\n",
        "        'is_correct': interaction['is_correct'],\n",
        "        'error_type': interaction.get('error_type'),\n",
        "        'error_explanation': error_explanation,\n",
        "        'mastery_before': interaction['mastery_before'],\n",
        "        'mastery_after': interaction['mastery_after'],\n",
        "        'time_spent_seconds': time_spent\n",
        "    }\n",
        "    \n",
        "    processed_interactions.append(complete_interaction)\n",
        "    success_count += 1\n",
        "    \n",
        "    # Rate limiting\n",
        "    time.sleep(RATE_LIMIT_DELAY)\n",
        "    \n",
        "    # Checkpoint e log de progresso\n",
        "    current_count = idx + 1\n",
        "    if current_count % CHECKPOINT_INTERVAL == 0:\n",
        "        save_checkpoint(processed_interactions, current_count)\n",
        "        elapsed = time.time() - start_time\n",
        "        rate = current_count / elapsed if elapsed > 0 else 0\n",
        "        eta = (len(interactions_bkt) - current_count) / rate if rate > 0 else 0\n",
        "        print(f\"Checkpoint: {current_count}/{len(interactions_bkt)} ({current_count/len(interactions_bkt)*100:.1f}%) - ETA: {eta/60:.1f}min\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"\\nProcessamento concluido!\")\n",
        "print(f\"  - Tempo total: {elapsed_time/60:.1f} min\")\n",
        "print(f\"  - Interacoes processadas: {len(processed_interactions)}\")\n",
        "print(f\"  - Taxa: {len(processed_interactions)/elapsed_time:.2f} int/s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Salvamento dos Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint removido (processamento completo)\n",
            "\n",
            "Resultados salvos em: data/output/notebooks/geracao_respostas_llm/interactions_complete.json\n",
            "Tamanho do arquivo: 3.27 MB\n"
          ]
        }
      ],
      "source": [
        "# Calcula metricas de qualidade\n",
        "total_interactions = len(processed_interactions)\n",
        "correct_count = sum(1 for i in processed_interactions if i['is_correct'])\n",
        "accuracy = correct_count / total_interactions if total_interactions > 0 else 0\n",
        "\n",
        "error_distribution = {}\n",
        "for interaction in processed_interactions:\n",
        "    error_type = interaction.get('error_type')\n",
        "    if error_type:\n",
        "        error_distribution[error_type] = error_distribution.get(error_type, 0) + 1\n",
        "\n",
        "student_counts = {}\n",
        "for interaction in processed_interactions:\n",
        "    sid = interaction['student_id']\n",
        "    student_counts[sid] = student_counts.get(sid, 0) + 1\n",
        "\n",
        "valid_responses = sum(1 for i in processed_interactions if i.get('response') and len(i['response']) > 5)\n",
        "\n",
        "# Monta output final\n",
        "output_data = {\n",
        "    \"metadata\": {\n",
        "        \"description\": \"Conjunto de interacoes simuladas com respostas geradas por LLM para estudantes SINKT\",\n",
        "        \"version\": \"2.0.0\",\n",
        "        \"created_at\": datetime.now().isoformat(),\n",
        "        \"llm_model\": MODEL_NAME,\n",
        "        \"llm_temperature\": TEMPERATURE,\n",
        "        \"total_interactions\": total_interactions,\n",
        "        \"total_students\": len(student_counts),\n",
        "        \"avg_interactions_per_student\": total_interactions / len(student_counts) if student_counts else 0,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"error_types\": list(ERROR_TYPE_DESCRIPTIONS.keys()),\n",
        "        \"quality_metrics\": {\n",
        "            \"total_interactions\": total_interactions,\n",
        "            \"total_students\": len(student_counts),\n",
        "            \"avg_interactions_per_student\": total_interactions / len(student_counts) if student_counts else 0,\n",
        "            \"correct_interactions\": correct_count,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"error_distribution\": error_distribution,\n",
        "            \"interactions_per_student_stats\": {\n",
        "                \"min\": min(student_counts.values()) if student_counts else 0,\n",
        "                \"max\": max(student_counts.values()) if student_counts else 0,\n",
        "                \"mean\": total_interactions / len(student_counts) if student_counts else 0\n",
        "            },\n",
        "            \"response_quality\": {\n",
        "                \"total_responses\": total_interactions,\n",
        "                \"empty_responses\": total_interactions - valid_responses,\n",
        "                \"valid_responses\": valid_responses,\n",
        "                \"validity_percentage\": (valid_responses / total_interactions * 100) if total_interactions else 0\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"interactions\": processed_interactions\n",
        "}\n",
        "\n",
        "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "remove_checkpoint()\n",
        "\n",
        "print(f\"\\nResultados salvos em: {OUTPUT_FILE}\")\n",
        "print(f\"Tamanho do arquivo: {os.path.getsize(OUTPUT_FILE) / (1024*1024):.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análise de Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "GERACAO DE RESPOSTAS LLM CONCLUIDA\n",
            "======================================================================\n",
            "\n",
            "Estatisticas Gerais:\n",
            "  - Total de interacoes: 4499\n",
            "  - Estudantes: 100\n",
            "  - Acuracia: 41.7%\n",
            "  - Respostas validas: 4499 (100.0%)\n",
            "\n",
            "Distribuicao de Erros:\n",
            "  - careless: 520 (19.8%)\n",
            "  - incomplete: 533 (20.3%)\n",
            "  - misconception: 496 (18.9%)\n",
            "  - misunderstanding: 512 (19.5%)\n",
            "  - slip: 561 (21.4%)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GERACAO DE RESPOSTAS LLM CONCLUIDA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nEstatisticas Gerais:\")\n",
        "print(f\"  - Total de interacoes: {total_interactions}\")\n",
        "print(f\"  - Estudantes: {len(student_counts)}\")\n",
        "print(f\"  - Acuracia: {accuracy:.1%}\")\n",
        "print(f\"  - Respostas validas: {valid_responses} ({valid_responses/total_interactions*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nDistribuicao de Erros:\")\n",
        "for error_type, count in sorted(error_distribution.items()):\n",
        "    pct = count / (total_interactions - correct_count) * 100 if (total_interactions - correct_count) > 0 else 0\n",
        "    print(f\"  - {error_type}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exemplos de Interações Geradas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Exemplos de Interacoes Geradas:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[ACERTO] student_0000 - concept_001_q3\n",
            "  Tipo: multiple_choice\n",
            "  Resposta: Opcao A\n",
            "\n",
            "[ACERTO] student_0000 - concept_088_q4\n",
            "  Tipo: descriptive\n",
            "  Resposta: Deepin é uma distribuição Linux que se destaca pela sua interface bonita e fácil de usar, o que ajuda muito quem está começando ou quer algo mais intu...\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[ERRO: slip] student_0000 - concept_055_q4\n",
            "  Tipo: descriptive\n",
            "  Resposta: O /tmp é um diretório usado para armazenar arquivos temporários criados pelos programas durante a execução. Ele é importante porque ajuda a guardar da...\n",
            "  Justificativa: O estudante errou ao afirmar que o uso do /tmp evita que a memória do sistema fique cheia, confundindo armazenamento temporário em disco com gerenciamento de memória RAM. É necessário reforçar o conceito de que /tmp é um diretório em disco para arquivos temporários, não relacionado diretamente à memória volátil do sistema.\n",
            "\n",
            "[ERRO: misconception] student_0000 - concept_177_q1\n",
            "  Tipo: multiple_choice\n",
            "  Resposta: Opcao B\n",
            "  Justificativa: O estudante confundiu o conceito de \"root\" com outro termo similar, demonstrando falta de compreensão sobre o papel do superusuário. É necessário reforçar que o usuário root possui permissões totais e funções administrativas no sistema.\n",
            "\n",
            "[ERRO: careless] student_0000 - concept_262_q4\n",
            "  Tipo: descriptive\n",
            "  Resposta: A hierarquia de diretórios é tipo uma lista de pastas que ficam todas no mesmo nível, tipo sem subpastas. No Linux, a estrutura de diretórios é meio b...\n",
            "  Justificativa: O estudante errou ao descrever a hierarquia de diretórios como uma lista plana e a estrutura do Linux como desorganizada, demonstrando falta de atenção aos conceitos. É necessário reforçar o entendimento da organização em árvore da hierarquia de diretórios e as regras definidas pela Estrutura de Diretórios do GNU/Linux.\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nExemplos de Interacoes Geradas:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Exemplo de acerto\n",
        "correct_examples = [i for i in processed_interactions if i['is_correct']][:2]\n",
        "for ex in correct_examples:\n",
        "    print(f\"\\n[ACERTO] {ex['student_id']} - {ex['question_id']}\")\n",
        "    print(f\"  Tipo: {ex['question_type']}\")\n",
        "    print(f\"  Resposta: {ex['response'][:150]}...\" if len(ex['response']) > 150 else f\"  Resposta: {ex['response']}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "\n",
        "# Exemplos de erro (um de cada tipo)\n",
        "shown_types = set()\n",
        "for ex in processed_interactions:\n",
        "    if not ex['is_correct'] and ex.get('error_type') and ex['error_type'] not in shown_types:\n",
        "        print(f\"\\n[ERRO: {ex['error_type']}] {ex['student_id']} - {ex['question_id']}\")\n",
        "        print(f\"  Tipo: {ex['question_type']}\")\n",
        "        print(f\"  Resposta: {ex['response'][:150]}...\" if len(ex['response']) > 150 else f\"  Resposta: {ex['response']}\")\n",
        "        print(f\"  Justificativa: {ex.get('error_explanation', 'N/A')}\")\n",
        "        shown_types.add(ex['error_type'])\n",
        "        if len(shown_types) >= 3:\n",
        "            break\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
