{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Simulação de Interações (BKT)\n",
    "\n",
    "Este notebook implementa a **Etapa 3** do pipeline SINKT: geração de interações simuladas usando apenas o modelo BKT.\n",
    "\n",
    "## Objetivo\n",
    "Gerar sequências de interações (acertos/erros) para cada estudante baseado nos parâmetros cognitivos e BKT, sem geração de respostas textuais.\n",
    "\n",
    "## Saída\n",
    "- `data/output/notebooks/simulacao_interacoes/interactions_bkt.json`: Arquivo JSON contendo todas as interações simuladas (sem respostas LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bibliotecas importadas com sucesso\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import time\n",
    "\n",
    "print(\"✅ Bibliotecas importadas com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dados carregados:\n",
      "  - Perfis: 6\n",
      "  - Estudantes: 100\n",
      "  - Questões: 680\n",
      "  - Conceitos: 251\n"
     ]
    }
   ],
   "source": [
    "with open('data/output/notebooks/geracao_perfis/profiles.json', 'r', encoding='utf-8') as f:\n",
    "    profiles_data = json.load(f)\n",
    "profiles = profiles_data['profiles']\n",
    "\n",
    "with open('data/output/notebooks/geracao_estudantes/students.json', 'r', encoding='utf-8') as f:\n",
    "    students_data = json.load(f)\n",
    "students = students_data['students']\n",
    "\n",
    "with open('data/json/questions_graph.json', 'r', encoding='utf-8') as f:\n",
    "    questions_data = json.load(f)\n",
    "questions = questions_data.get('questions', [])\n",
    "\n",
    "with open('data/json/concepts_graph.json', 'r', encoding='utf-8') as f:\n",
    "    concepts_data = json.load(f)\n",
    "concepts = concepts_data.get('concepts', [])\n",
    "\n",
    "print(f\"✅ Dados carregados:\")\n",
    "print(f\"  - Perfis: {len(profiles)}\")\n",
    "print(f\"  - Estudantes: {len(students)}\")\n",
    "print(f\"  - Questões: {len(questions)}\")\n",
    "print(f\"  - Conceitos: {len(concepts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração de Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuracoes:\n",
      "  - Interacoes por estudante: 30-60\n",
      "  - Checkpoint a cada: 10 estudantes\n",
      "  - Seed: 42\n"
     ]
    }
   ],
   "source": [
    "MIN_INTERACTIONS_PER_STUDENT = 30\n",
    "MAX_INTERACTIONS_PER_STUDENT = 60\n",
    "SEED = 42\n",
    "\n",
    "CHECKPOINT_INTERVAL = 10\n",
    "CHECKPOINT_FILE = 'data/output/notebooks/simulacao_interacoes/interactions_bkt_checkpoint.json'\n",
    "\n",
    "ERROR_TYPES = [\n",
    "    'misconception',\n",
    "    'careless',\n",
    "    'slip',\n",
    "    'incomplete',\n",
    "    'misunderstanding'\n",
    "]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print(f\"✅ Configuracoes:\")\n",
    "print(f\"  - Interacoes por estudante: {MIN_INTERACTIONS_PER_STUDENT}-{MAX_INTERACTIONS_PER_STUDENT}\")\n",
    "print(f\"  - Checkpoint a cada: {CHECKPOINT_INTERVAL} estudantes\")\n",
    "print(f\"  - Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcoes BKT\n",
    "\n",
    "- Usa `current_mastery` na probabilidade (nao `mastery_init_level`)\n",
    "- Bayesiano padrao seguido de transicao de aprendizagem\n",
    "- Fatores cognitivos afetam `slip_eff` e `guess_eff`\n",
    "- Ruido de consistencia aplicado na probabilidade final\n",
    "- Decay temporal so com gap > 6h, coeficiente reduzido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes para ajuste fino dos pesos cognitivos\n",
    "DECAY_THRESHOLD_SECONDS = 6 * 3600  # 6 horas em segundos\n",
    "DECAY_COEFFICIENT = 0.02  # Reduzido de 0.1\n",
    "\n",
    "def calculate_effective_params(student_params: Dict, question_difficulty: float) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calcula slip, guess e learn_rate efetivos baseado nos fatores cognitivos.\n",
    "    \n",
    "    Correção C: Fatores cognitivos afetam slip/guess, não mastery diretamente.\n",
    "    \"\"\"\n",
    "    # Parâmetros base do estudante\n",
    "    slip_base = student_params.get('slip', 0.1)\n",
    "    guess_base = student_params.get('guess', 0.15)\n",
    "    learn_rate_base = student_params.get('learn_rate', 0.05)\n",
    "    \n",
    "    # Fatores cognitivos\n",
    "    logic_skill = student_params.get('logic_skill', 0.5)\n",
    "    reading_skill = student_params.get('reading_skill', 0.5)\n",
    "    tech_familiarity = student_params.get('tech_familiarity', 0.5)\n",
    "    learning_consistency = student_params.get('learning_consistency', 0.5)\n",
    "    \n",
    "    # guess_eff: tech e logic aumentam chance de \"se virar\" mesmo sem dominar\n",
    "    # Dificuldade alta reduz o guess efetivo\n",
    "    guess_eff = guess_base * (0.85 + 0.20 * tech_familiarity + 0.15 * logic_skill - 0.20 * question_difficulty)\n",
    "    guess_eff = np.clip(guess_eff, 0.0, 0.35)\n",
    "    \n",
    "    # slip_eff: baixa consistencia e reading aumentam erro tipo \"descuido/misunderstanding\"\n",
    "    # Dificuldade alta aumenta slip\n",
    "    slip_eff = slip_base * (0.90 + 0.40 * (1 - learning_consistency) + 0.20 * question_difficulty + 0.20 * (1 - reading_skill))\n",
    "    slip_eff = np.clip(slip_eff, 0.0, 0.45)\n",
    "    \n",
    "    # learn_eff: tech ajuda a converter tentativa em aprendizado\n",
    "    learn_eff = learn_rate_base * (0.85 + 0.15 * tech_familiarity)\n",
    "    learn_eff = np.clip(learn_eff, 0.0, 0.35)\n",
    "    \n",
    "    return slip_eff, guess_eff, learn_eff\n",
    "\n",
    "def calculate_response_probability(current_mastery: float, student_params: Dict, \n",
    "                                   question_difficulty: float) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calcula probabilidade de resposta correta usando formula BKT classica.\n",
    "    \n",
    "    Formula BKT: P(correct) = P(L)*(1-slip) + (1-P(L))*guess\n",
    "    \n",
    "    Usa current_mastery (nao mastery_init_level)\n",
    "    Fatores cognitivos afetam slip/guess efetivos\n",
    "    Ruido de consistencia aplicado na prob final, nao no mastery\n",
    "    \"\"\"\n",
    "    # Calcula parametros efetivos baseado em fatores cognitivos\n",
    "    slip_eff, guess_eff, learn_eff = calculate_effective_params(student_params, question_difficulty)\n",
    "    \n",
    "    # Ajuste leve do mastery por dificuldade (pequeno efeito)\n",
    "    k_d = 0.15  # Coeficiente de dificuldade\n",
    "    m_adj = np.clip(current_mastery - k_d * question_difficulty, 0.0, 1.0)\n",
    "    \n",
    "    # Formula BKT classica: P(correct) = P(L)*(1-slip) + (1-P(L))*guess\n",
    "    prob = m_adj * (1 - slip_eff) + (1 - m_adj) * guess_eff\n",
    "    \n",
    "    # Ruido de consistencia na probabilidade final (nao no mastery)\n",
    "    learning_consistency = student_params.get('learning_consistency', 0.5)\n",
    "    if learning_consistency < 0.7:\n",
    "        noise_sigma = (0.7 - learning_consistency) * 0.1  # Ruido proporcional a inconsistencia\n",
    "        noise = np.random.normal(0, noise_sigma)\n",
    "        prob += noise\n",
    "    \n",
    "    prob = np.clip(prob, 0.0, 1.0)\n",
    "    \n",
    "    return prob, slip_eff, guess_eff\n",
    "\n",
    "def update_mastery_bkt(current_mastery: float, is_correct: bool, \n",
    "                       slip: float, guess: float, learn_rate: float,\n",
    "                       memory_capacity: float = 0.5, time_gap: float = 0) -> float:\n",
    "    \"\"\"\n",
    "    Atualiza o dominio do estudante usando update BKT padrao (Bayes + learn).\n",
    "    \n",
    "    Update Bayesiano correto seguido de transicao de aprendizagem.\n",
    "    Decay temporal so com gap > 6h, coeficiente reduzido.\n",
    "    \n",
    "    Referencia: Corbett & Anderson (1995) - Knowledge Tracing\n",
    "    \"\"\"\n",
    "    P_L = current_mastery\n",
    "    \n",
    "    # Update Bayesiano dado observacao (acerto/erro)\n",
    "    if is_correct:\n",
    "        # P(L|correct) = P(L)*(1-slip) / [P(L)*(1-slip) + (1-P(L))*guess]\n",
    "        numerator = P_L * (1 - slip)\n",
    "        denominator = P_L * (1 - slip) + (1 - P_L) * guess\n",
    "        if denominator > 0:\n",
    "            P_L_given_obs = numerator / denominator\n",
    "        else:\n",
    "            P_L_given_obs = P_L\n",
    "    else:\n",
    "        # P(L|wrong) = P(L)*slip / [P(L)*slip + (1-P(L))*(1-guess)]\n",
    "        numerator = P_L * slip\n",
    "        denominator = P_L * slip + (1 - P_L) * (1 - guess)\n",
    "        if denominator > 0:\n",
    "            P_L_given_obs = numerator / denominator\n",
    "        else:\n",
    "            P_L_given_obs = P_L\n",
    "    \n",
    "    # Transicao de aprendizagem: P(L_next) = P(L|obs) + (1 - P(L|obs)) * learn_rate\n",
    "    P_L_next = P_L_given_obs + (1 - P_L_given_obs) * learn_rate\n",
    "    \n",
    "    # Correcao E: Decay temporal so se gap > 6h (DECAY_THRESHOLD_SECONDS)\n",
    "    if time_gap > DECAY_THRESHOLD_SECONDS:\n",
    "        # Decay proporcional a (1 - memory_capacity) e ao tempo\n",
    "        time_factor = min((time_gap - DECAY_THRESHOLD_SECONDS) / 86400, 1.0)  # Normalizado por 24h\n",
    "        decay_factor = 1 - (1 - memory_capacity) * DECAY_COEFFICIENT * time_factor\n",
    "        decay_factor = max(0.5, decay_factor)  # Nunca decai mais que 50%\n",
    "        P_L_next *= decay_factor\n",
    "    \n",
    "    return np.clip(P_L_next, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 10/100 estudantes processados (423 interacoes)\n",
      "Checkpoint: 20/100 estudantes processados (919 interacoes)\n",
      "Checkpoint: 30/100 estudantes processados (1345 interacoes)\n",
      "Checkpoint: 40/100 estudantes processados (1777 interacoes)\n",
      "Checkpoint: 50/100 estudantes processados (2257 interacoes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 60/100 estudantes processados (2738 interacoes)\n",
      "Checkpoint: 70/100 estudantes processados (3149 interacoes)\n",
      "Checkpoint: 80/100 estudantes processados (3579 interacoes)\n",
      "Checkpoint: 90/100 estudantes processados (4059 interacoes)\n",
      "Checkpoint: 100/100 estudantes processados (4499 interacoes)\n",
      "\n",
      "✅ Geracao de interacoes concluida!\n",
      "  - Tempo total: 0.30s\n",
      "  - Total de interacoes: 4499\n",
      "  - Media por estudante: 45.0\n"
     ]
    }
   ],
   "source": [
    "interactions = []\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, student in enumerate(students, 1):\n",
    "    student_id = student['id']\n",
    "    num_interactions = np.random.randint(MIN_INTERACTIONS_PER_STUDENT, MAX_INTERACTIONS_PER_STUDENT + 1)\n",
    "    \n",
    "    # Inicializa com mastery_init_level, mas usa current_mastery na probabilidade\n",
    "    current_mastery = student.get('mastery_init_level', 0.5)\n",
    "    last_interaction_time = datetime.now() - timedelta(days=30)\n",
    "    \n",
    "    # Calcula learn_rate efetivo uma vez por estudante (baseado em tech_familiarity)\n",
    "    _, _, learn_eff = calculate_effective_params(student, 0.5)\n",
    "    \n",
    "    for i in range(num_interactions):\n",
    "        question = random.choice(questions)\n",
    "        question_id = question['id']\n",
    "        question_difficulty = question.get('difficulty', 0.5)\n",
    "        \n",
    "        time_gap = (datetime.now() - last_interaction_time).total_seconds()\n",
    "        \n",
    "        # Passa current_mastery (nao mastery_init_level)\n",
    "        prob_correct, slip_eff, guess_eff = calculate_response_probability(\n",
    "            current_mastery, student, question_difficulty\n",
    "        )\n",
    "        # Converte np.bool_ para bool Python nativo (JSON serializable)\n",
    "        is_correct = bool(np.random.random() < prob_correct)\n",
    "        \n",
    "        error_type = None\n",
    "        if not is_correct:\n",
    "            error_type = random.choice(ERROR_TYPES)\n",
    "        \n",
    "        interaction = {\n",
    "            'student_id': student_id,\n",
    "            'question_id': question_id,\n",
    "            'is_correct': is_correct,\n",
    "            'mastery_before': round(float(current_mastery), 4),\n",
    "            'probability': round(float(prob_correct), 4),\n",
    "            'slip_eff': round(float(slip_eff), 4),\n",
    "            'guess_eff': round(float(guess_eff), 4),\n",
    "            'error_type': error_type,\n",
    "            'timestamp': (datetime.now() - timedelta(days=30-i)).isoformat()\n",
    "        }\n",
    "        \n",
    "        # Update BKT padrao (Bayes + learn)\n",
    "        current_mastery = update_mastery_bkt(\n",
    "            current_mastery,\n",
    "            is_correct,\n",
    "            slip_eff,\n",
    "            guess_eff,\n",
    "            learn_eff,\n",
    "            student.get('memory_capacity', 0.5),\n",
    "            time_gap\n",
    "        )\n",
    "        \n",
    "        interaction['mastery_after'] = round(float(current_mastery), 4)\n",
    "        interactions.append(interaction)\n",
    "        \n",
    "        last_interaction_time = datetime.now() - timedelta(days=30-i)\n",
    "    \n",
    "    if idx % CHECKPOINT_INTERVAL == 0:\n",
    "        checkpoint_data = {\n",
    "            'processed_students': idx,\n",
    "            'total_interactions': len(interactions),\n",
    "            'interactions': interactions\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(CHECKPOINT_FILE), exist_ok=True)\n",
    "        with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Checkpoint: {idx}/{len(students)} estudantes processados ({len(interactions)} interacoes)\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n✅ Geracao de interacoes concluida!\")\n",
    "print(f\"  - Tempo total: {elapsed_time:.2f}s\")\n",
    "print(f\"  - Total de interacoes: {len(interactions)}\")\n",
    "print(f\"  - Media por estudante: {len(interactions)/len(students):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvamento das Interacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint removido (processamento completo)\n",
      "\n",
      "✅ Interacoes salvas em: data/output/notebooks/simulacao_interacoes/interactions_bkt.json\n",
      "Total de interacoes: 4499\n",
      "Tamanho do arquivo: 1.45 MB\n"
     ]
    }
   ],
   "source": [
    "output_data = {\n",
    "    \"metadata\": {\n",
    "        \"description\": \"Interacoes simuladas com BKT corrigido (Corbett & Anderson, 1995)\",\n",
    "        \"version\": \"4.0.0\",\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"total_interactions\": len(interactions),\n",
    "        \"total_students\": len(students),\n",
    "        \"avg_interactions_per_student\": len(interactions) / len(students),\n",
    "        \"seed\": SEED,\n",
    "        \"bkt_corrections\": {\n",
    "            \"A\": \"Usa current_mastery na probabilidade (nao mastery_init)\",\n",
    "            \"B\": \"Update Bayesiano padrao + transicao de aprendizagem\",\n",
    "            \"C\": \"Fatores cognitivos afetam slip/guess efetivos\",\n",
    "            \"D\": \"Ruido de consistencia na prob final (nao no mastery)\",\n",
    "            \"E\": \"Decay temporal so com gap > 6h, coeficiente reduzido\"\n",
    "        }\n",
    "    },\n",
    "    \"interactions\": interactions\n",
    "}\n",
    "\n",
    "output_file = 'data/output/notebooks/simulacao_interacoes/interactions_bkt.json'\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    os.remove(CHECKPOINT_FILE)\n",
    "    print(\"Checkpoint removido (processamento completo)\")\n",
    "\n",
    "print(f\"\\n✅ Interacoes salvas em: {output_file}\")\n",
    "print(f\"Total de interacoes: {len(interactions)}\")\n",
    "print(f\"Tamanho do arquivo: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estatísticas Gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Estatísticas Gerais das Interações:\n",
      "\n",
      "  Total de Interações: 4499\n",
      "  Corretas: 1860 (41.3%)\n",
      "  Incorretas: 2639 (58.7%)\n",
      "\n",
      "  Mastery (Domínio):\n",
      "    Média: 0.484\n",
      "    Mediana: 0.250\n",
      "    Desvio: 0.442\n",
      "    Range: [0.018, 1.000]\n"
     ]
    }
   ],
   "source": [
    "def analyze_general_statistics(interactions: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Calcula estatísticas gerais das interações.\"\"\"\n",
    "    total_interactions = len(interactions)\n",
    "    correct_interactions = sum(1 for i in interactions if i['is_correct'])\n",
    "    \n",
    "    stats = {\n",
    "        'total_interactions': total_interactions,\n",
    "        'correct_interactions': correct_interactions,\n",
    "        'incorrect_interactions': total_interactions - correct_interactions,\n",
    "        'accuracy': correct_interactions / total_interactions if total_interactions > 0 else 0\n",
    "    }\n",
    "    \n",
    "    masteries = [i['mastery_after'] for i in interactions]\n",
    "    stats['mastery_statistics'] = {\n",
    "        'mean': np.mean(masteries),\n",
    "        'median': np.median(masteries),\n",
    "        'std': np.std(masteries),\n",
    "        'min': np.min(masteries),\n",
    "        'max': np.max(masteries)\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "general_stats = analyze_general_statistics(interactions)\n",
    "\n",
    "print(\"\\n✅ Estatísticas Gerais das Interações:\\n\")\n",
    "print(f\"  Total de Interações: {general_stats['total_interactions']}\")\n",
    "print(f\"  Corretas: {general_stats['correct_interactions']} ({general_stats['accuracy']:.1%})\")\n",
    "print(f\"  Incorretas: {general_stats['incorrect_interactions']} ({1-general_stats['accuracy']:.1%})\")\n",
    "print(f\"\\n  Mastery (Domínio):\")\n",
    "print(f\"    Média: {general_stats['mastery_statistics']['mean']:.3f}\")\n",
    "print(f\"    Mediana: {general_stats['mastery_statistics']['median']:.3f}\")\n",
    "print(f\"    Desvio: {general_stats['mastery_statistics']['std']:.3f}\")\n",
    "print(f\"    Range: [{general_stats['mastery_statistics']['min']:.3f}, {general_stats['mastery_statistics']['max']:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição de Erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Análise de Distribuição de Erros:\n",
      "\n",
      "  Total de Erros: 2639\n",
      "\n",
      "  Distribuição por Tipo:\n",
      "    - misconception: 524 (19.9%)\n",
      "    - slip: 547 (20.7%)\n",
      "    - careless: 515 (19.5%)\n",
      "    - misunderstanding: 516 (19.6%)\n",
      "    - incomplete: 537 (20.3%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_error_distribution(interactions: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Analisa distribuição de tipos de erro.\"\"\"\n",
    "    error_distribution = Counter()\n",
    "    \n",
    "    for interaction in interactions:\n",
    "        if interaction['error_type']:\n",
    "            error_distribution[interaction['error_type']] += 1\n",
    "    \n",
    "    total_errors = sum(error_distribution.values())\n",
    "    \n",
    "    error_stats = {}\n",
    "    for error_type, count in error_distribution.items():\n",
    "        error_stats[error_type] = {\n",
    "            'count': count,\n",
    "            'percentage': (count / total_errors * 100) if total_errors > 0 else 0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'total_errors': total_errors,\n",
    "        'error_distribution': error_stats\n",
    "    }\n",
    "\n",
    "error_analysis = analyze_error_distribution(interactions)\n",
    "\n",
    "print(\"\\n✅ Análise de Distribuição de Erros:\\n\")\n",
    "print(f\"  Total de Erros: {error_analysis['total_errors']}\")\n",
    "print(f\"\\n  Distribuição por Tipo:\")\n",
    "for error_type, stats in error_analysis['error_distribution'].items():\n",
    "    print(f\"    - {error_type}: {stats['count']} ({stats['percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desempenho por Perfil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Desempenho por Perfil Cognitivo:\n",
      "\n",
      "  balanced:\n",
      "    Estudantes: 30\n",
      "    Acurácia: 43.0% (±24.6%)\n",
      "    Domínio: 0.499 (±0.319)\n",
      "\n",
      "  careful:\n",
      "    Estudantes: 20\n",
      "    Acurácia: 42.5% (±35.6%)\n",
      "    Domínio: 0.457 (±0.408)\n",
      "\n",
      "  intuitive:\n",
      "    Estudantes: 10\n",
      "    Acurácia: 37.7% (±24.6%)\n",
      "    Domínio: 0.419 (±0.315)\n",
      "\n",
      "  logical:\n",
      "    Estudantes: 10\n",
      "    Acurácia: 30.5% (±26.8%)\n",
      "    Domínio: 0.401 (±0.336)\n",
      "\n",
      "  quick_learner:\n",
      "    Estudantes: 20\n",
      "    Acurácia: 49.5% (±24.2%)\n",
      "    Domínio: 0.657 (±0.304)\n",
      "\n",
      "  struggling:\n",
      "    Estudantes: 10\n",
      "    Acurácia: 25.0% (±10.1%)\n",
      "    Domínio: 0.186 (±0.152)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_performance_by_profile(interactions: List[Dict], students: List) -> Dict[str, Any]:\n",
    "    \"\"\"Analisa desempenho agrupado por perfil cognitivo.\"\"\"\n",
    "    profile_data = defaultdict(lambda: {\n",
    "        'students': [],\n",
    "        'accuracies': [],\n",
    "        'masteries': []\n",
    "    })\n",
    "    \n",
    "    student_interactions = defaultdict(list)\n",
    "    for interaction in interactions:\n",
    "        student_interactions[interaction['student_id']].append(interaction)\n",
    "    \n",
    "    for student in students:\n",
    "        student_id = student['id']\n",
    "        profile_id = student['profile_id']\n",
    "        student_ints = student_interactions.get(student_id, [])\n",
    "        \n",
    "        if not student_ints:\n",
    "            continue\n",
    "        \n",
    "        accuracy = sum(1 for i in student_ints if i['is_correct']) / len(student_ints)\n",
    "        avg_mastery = np.mean([i['mastery_after'] for i in student_ints])\n",
    "        \n",
    "        profile_data[profile_id]['students'].append(student_id)\n",
    "        profile_data[profile_id]['accuracies'].append(accuracy)\n",
    "        profile_data[profile_id]['masteries'].append(avg_mastery)\n",
    "    \n",
    "    profile_stats = {}\n",
    "    for profile_id, data in profile_data.items():\n",
    "        if data['accuracies']:\n",
    "            profile_stats[profile_id] = {\n",
    "                'num_students': len(data['students']),\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(data['accuracies']),\n",
    "                    'std': np.std(data['accuracies'])\n",
    "                },\n",
    "                'mastery': {\n",
    "                    'mean': np.mean(data['masteries']),\n",
    "                    'std': np.std(data['masteries'])\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    return profile_stats\n",
    "\n",
    "profile_performance = analyze_performance_by_profile(interactions, students)\n",
    "\n",
    "print(\"\\n✅ Desempenho por Perfil Cognitivo:\\n\")\n",
    "for profile_id, stats in sorted(profile_performance.items()):\n",
    "    print(f\"  {profile_id}:\")\n",
    "    print(f\"    Estudantes: {stats['num_students']}\")\n",
    "    print(f\"    Acurácia: {stats['accuracy']['mean']:.1%} (±{stats['accuracy']['std']:.1%})\")\n",
    "    print(f\"    Domínio: {stats['mastery']['mean']:.3f} (±{stats['mastery']['std']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlação entre Parâmetros e Desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Correlação entre Parâmetros e Desempenho:\n",
      "\n",
      "  Parâmetro            | Corr. Acurácia | Corr. Domínio\n",
      "  ------------------------------------------------------------\n",
      "  memory_capacity      |  0.252         |  0.345\n",
      "  mastery_init         |  0.210         |  0.290\n",
      "  tech_familiarity     |  0.201         |  0.328\n",
      "  learn_rate           |  0.199         |  0.322\n",
      "  reading_skill        |  0.170         |  0.165\n",
      "  learning_consistency |  0.167         |  0.219\n",
      "  logic_skill          |  0.086         |  0.192\n",
      "  slip                 | -0.058         | -0.054\n",
      "  guess                | -0.046         | -0.127\n",
      "\n",
      "  Top 5 Fatores Mais Importantes:\n",
      "    1. memory_capacity\n",
      "    2. mastery_init\n",
      "    3. tech_familiarity\n",
      "    4. learn_rate\n",
      "    5. reading_skill\n"
     ]
    }
   ],
   "source": [
    "def analyze_parameter_correlations(interactions: List[Dict], students: List) -> Dict[str, Any]:\n",
    "    \"\"\"Analisa correlação entre parâmetros dos estudantes e seu desempenho.\"\"\"\n",
    "    student_interactions = defaultdict(list)\n",
    "    for interaction in interactions:\n",
    "        student_interactions[interaction['student_id']].append(interaction)\n",
    "    \n",
    "    data = []\n",
    "    for student in students:\n",
    "        student_id = student['id']\n",
    "        student_ints = student_interactions.get(student_id, [])\n",
    "        if not student_ints:\n",
    "            continue\n",
    "        \n",
    "        accuracy = sum(1 for i in student_ints if i['is_correct']) / len(student_ints)\n",
    "        avg_mastery = np.mean([i['mastery_after'] for i in student_ints])\n",
    "        \n",
    "        data.append({\n",
    "            'student_id': student_id,\n",
    "            'accuracy': accuracy,\n",
    "            'avg_mastery': avg_mastery,\n",
    "            'learn_rate': student.get('learn_rate', 0),\n",
    "            'logic_skill': student.get('logic_skill', 0),\n",
    "            'reading_skill': student.get('reading_skill', 0),\n",
    "            'tech_familiarity': student.get('tech_familiarity', 0),\n",
    "            'memory_capacity': student.get('memory_capacity', 0),\n",
    "            'learning_consistency': student.get('learning_consistency', 0),\n",
    "            'mastery_init': student.get('mastery_init_level', 0),\n",
    "            'slip': student.get('slip', 0),\n",
    "            'guess': student.get('guess', 0)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    correlations = {}\n",
    "    for param in ['learn_rate', 'logic_skill', 'reading_skill', 'tech_familiarity',\n",
    "                  'memory_capacity', 'learning_consistency', 'mastery_init', 'slip', 'guess']:\n",
    "        if param in df.columns:\n",
    "            corr_with_accuracy = df[param].corr(df['accuracy'])\n",
    "            corr_with_mastery = df[param].corr(df['avg_mastery'])\n",
    "            correlations[param] = {\n",
    "                'correlation_with_accuracy': round(corr_with_accuracy, 3),\n",
    "                'correlation_with_mastery': round(corr_with_mastery, 3)\n",
    "            }\n",
    "    \n",
    "    sorted_by_accuracy = sorted(\n",
    "        correlations.items(),\n",
    "        key=lambda x: abs(x[1]['correlation_with_accuracy']),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'correlations': dict(sorted_by_accuracy),\n",
    "        'top_5_factors': [f[0] for f in sorted_by_accuracy[:5]]\n",
    "    }\n",
    "\n",
    "correlations = analyze_parameter_correlations(interactions, students)\n",
    "\n",
    "print(\"\\n✅ Correlação entre Parâmetros e Desempenho:\\n\")\n",
    "print(\"  Parâmetro            | Corr. Acurácia | Corr. Domínio\")\n",
    "print(\"  \" + \"-\" * 60)\n",
    "for param, corrs in list(correlations['correlations'].items()):\n",
    "    acc_corr = corrs['correlation_with_accuracy']\n",
    "    mas_corr = corrs['correlation_with_mastery']\n",
    "    print(f\"  {param:20s} | {acc_corr:>6.3f}         | {mas_corr:>6.3f}\")\n",
    "\n",
    "print(f\"\\n  Top 5 Fatores Mais Importantes:\")\n",
    "for i, factor in enumerate(correlations['top_5_factors'], 1):\n",
    "    print(f\"    {i}. {factor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo da Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "✅ SIMULAÇÃO DE INTERAÇÕES BKT CONCLUÍDA COM SUCESSO!\n",
      "======================================================================\n",
      "\n",
      "  Arquivo gerado: data/output/notebooks/simulacao_interacoes/interactions_bkt.json\n",
      "\n",
      "  Resumo:\n",
      "    - Total de interações: 4499\n",
      "    - Estudantes: 100\n",
      "    - Acurácia: 41.3%\n",
      "\n",
      "✅ Próximo passo: Execute o notebook '04_geracao_respostas_llm.ipynb'\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ SIMULAÇÃO DE INTERAÇÕES BKT CONCLUÍDA COM SUCESSO!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n  Arquivo gerado: {output_file}\")\n",
    "print(f\"\\n  Resumo:\")\n",
    "print(f\"    - Total de interações: {len(interactions)}\")\n",
    "print(f\"    - Estudantes: {len(students)}\")\n",
    "print(f\"    - Acurácia: {general_stats['accuracy']:.1%}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
