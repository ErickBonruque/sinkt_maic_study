{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 1 - Pipeline de Extra√ß√£o de Conceitos (PDF -> Markdown -> LLM)\n",
    "\n",
    "Extra√ß√£o de conceitos t√©cnicos a partir do livro em PDF, utilizando uma abordagem de convers√£o intermedi√°ria para Markdown para preservar a hierarquia dos cap√≠tulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configura√ß√£o Inicial:** Importa as bibliotecas necess√°rias e carrega as vari√°veis de ambiente (chaves de API) para preparar o ambiente de execu√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    }
   ],
   "source": [
    "# Instala√ß√£o de depend√™ncias\n",
    "!pip install -q pymupdf4llm langchain langchain-openai langchain-anthropic pydantic tqdm python-dotenv ipywidgets\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pymupdf4llm\n",
    "# Usando tqdm padr√£o para evitar conflitos de widget\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Optional, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\n",
    "\n",
    "# Carregar vari√°veis de ambiente (.env)\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"AVISO: OPENAI_API_KEY n√£o encontrada no ambiente.\")\n",
    "if not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    print(\"AVISO: ANTHROPIC_API_KEY n√£o encontrada no ambiente. A etapa de indu√ß√£o de ontologia falhar√°.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convers√£o PDF para Markdown:** Converte o arquivo PDF original em texto estruturado Markdown, facilitando a leitura e extra√ß√£o de informa√ß√µes pelo LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_markdown_memory(pdf_path):\n",
    "    \"\"\"\n",
    "    Converte um arquivo PDF para Markdown mantendo o conte√∫do em mem√≥ria.\n",
    "    Utiliza pymupdf4llm para preservar a estrutura sem√¢ntica.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"Arquivo n√£o encontrado: {pdf_path}\")\n",
    "    \n",
    "    print(f\"Convertendo '{pdf_path}' para Markdown...\")\n",
    "    # pymupdf4llm.to_markdown retorna uma string com o conte√∫do em Markdown\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "    print(\"Convers√£o conclu√≠da.\")\n",
    "    return md_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segmenta√ß√£o de Cap√≠tulos:** Divide o texto Markdown completo em cap√≠tulos individuais, permitindo um processamento granular e isolado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_chapters(md_content):\n",
    "    \"\"\"\n",
    "    Segmenta o conte√∫do Markdown em cap√≠tulos baseando-se no padr√£o '# **N**'.\n",
    "    Ignora conte√∫do antes do primeiro cap√≠tulo numerado.\n",
    "    \"\"\"\n",
    "    # Regex para identificar o in√≠cio dos cap√≠tulos conforme padr√£o observado: # **<N√∫mero>**\n",
    "    # Captura o n√∫mero do cap√≠tulo\n",
    "    pattern = r'\\n# \\*\\*(\\d+)\\*\\*'\n",
    "    \n",
    "    # Encontrar todas as posi√ß√µes de in√≠cio de cap√≠tulo\n",
    "    matches = list(re.finditer(pattern, md_content))\n",
    "    \n",
    "    chapters = []\n",
    "    \n",
    "    if not matches:\n",
    "        print(\"Nenhum padr√£o de cap√≠tulo '# **N**' encontrado. Verifique o formato do Markdown.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Encontrados {len(matches)} marcadores de cap√≠tulo.\")\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        chapter_num = match.group(1)\n",
    "        start_pos = match.start()\n",
    "        \n",
    "        # O fim deste cap√≠tulo √© o in√≠cio do pr√≥ximo, ou o fim do arquivo\n",
    "        if i + 1 < len(matches):\n",
    "            end_pos = matches[i+1].start()\n",
    "        else:\n",
    "            end_pos = len(md_content)\n",
    "            \n",
    "        # Extrair conte√∫do\n",
    "        chapter_content = md_content[start_pos:end_pos].strip()\n",
    "        \n",
    "        # Tentar extrair um t√≠tulo da primeira linha ou linhas adjacentes para log/valida√ß√£o\n",
    "        lines = chapter_content.split('\\n')\n",
    "        header_line = lines[0]\n",
    "        # Remover o marcador para tentar pegar o resto como t√≠tulo\n",
    "        title_candidate = re.sub(r'# \\*\\*\\d+\\*\\*\\s*', '', header_line).strip()\n",
    "        \n",
    "        # Se o t√≠tulo n√£o estiver na mesma linha, verifica a pr√≥xima\n",
    "        if not title_candidate and len(lines) > 1:\n",
    "            next_line = lines[1].strip()\n",
    "            if next_line:\n",
    "                title_candidate = next_line\n",
    "        \n",
    "        if not title_candidate:\n",
    "            title_candidate = f\"Cap√≠tulo {chapter_num}\"\n",
    "\n",
    "        # Limita o tamanho do t√≠tulo para exibi√ß√£o\n",
    "        display_title = (title_candidate[:75] + '..') if len(title_candidate) > 75 else title_candidate\n",
    "        print(f\"  üìÑ Cap√≠tulo {chapter_num}: {display_title}\")\n",
    "        \n",
    "        chapters.append({\n",
    "            'chapter_id': chapter_num,\n",
    "            'title': title_candidate,\n",
    "            'content': chapter_content\n",
    "        })\n",
    "        \n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configura√ß√£o do Extrator:** Define os esquemas de dados (Schemas) e configura o modelo LLM e o prompt para a extra√ß√£o inicial de conceitos dos textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configura√ß√£o de Extra√ß√£o (gpt-4o-mini) conclu√≠da.\n"
     ]
    }
   ],
   "source": [
    "# --- Configura√ß√£o dos Schemas e LLM de Extra√ß√£o ---\n",
    "\n",
    "# Schema flex√≠vel para a primeira etapa (Extra√ß√£o em Massa)\n",
    "class Concept(BaseModel):\n",
    "    nome: str = Field(description=\"O nome exato do conceito t√©cnico, normalizado (ex: 'ls', '/etc/passwd').\")\n",
    "    tipo: str = Field(description=\"Categoria t√©cnica espec√≠fica inferida do texto. Seja detalhista (ex: 'Gerenciador de Pacotes', 'Vari√°vel de Ambiente', 'Daemon do Kernel').\")\n",
    "    definicao: str = Field(description=\"Uma defini√ß√£o t√©cnica focada na utilidade e fun√ß√£o dentro do sistema Linux.\")\n",
    "    capitulo_origem: str = Field(description=\"ID do cap√≠tulo de onde foi extra√≠do.\")\n",
    "\n",
    "class ConceptList(BaseModel):\n",
    "    conceitos: List[Concept]\n",
    "\n",
    "# Inicializa o modelo extrator (gpt-4o-mini - R√°pido e Eficiente)\n",
    "try:\n",
    "    llm_extractor = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao inicializar ChatOpenAI: {e}\")\n",
    "\n",
    "# Parser\n",
    "parser = PydanticOutputParser(pydantic_object=ConceptList)\n",
    "\n",
    "# Prompt Template para Extra√ß√£o Livre\n",
    "prompt_extractor = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Voc√™ √© um Especialista em Knowledge Tracing e Engenharia de Dados.\n",
    "    Sua tarefa √© extrair conceitos t√©cnicos (Knowledge Components) de material did√°tico.\n",
    "    \"\"\"),\n",
    "    (\"user\", \"\"\"Analise o texto do Cap√≠tulo {chapter_id} ({chapter_title}).\n",
    "\n",
    "    ### OBJETIVO:\n",
    "    Extraia TODOS os conceitos t√©cnicos relevantes.\n",
    "    \n",
    "    ### REGRAS:\n",
    "    1. **Canonicaliza√ß√£o de Nomes:**\n",
    "       - Comandos: Apenas o bin√°rio (ex: \"ls\", n√£o \"comando ls\").\n",
    "       - Diret√≥rios: Caminho absoluto (ex: \"/etc\").\n",
    "    2. **Tipagem Livre:**\n",
    "       - N√£o se restrinja a categorias fixas. Identifique o que o objeto √â no contexto (ex: \"Op√ß√£o de Boot\", \"Sistema de Arquivos\", \"Sigla\").\n",
    "    3. **Defini√ß√µes Ricas:** Explique a fun√ß√£o e utilidade t√©cnica.\n",
    "\n",
    "    {format_instructions}\n",
    "\n",
    "    ---\n",
    "    Texto:\n",
    "    {content}\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "# Chain de Extra√ß√£o\n",
    "chain_extractor = prompt_extractor | llm_extractor | parser\n",
    "\n",
    "print(\"Configura√ß√£o de Extra√ß√£o (gpt-4o-mini) conclu√≠da.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline de Extra√ß√£o em Massa:** Executa a extra√ß√£o de conceitos em todos os cap√≠tulos do livro de forma paralela para otimizar o tempo de processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertendo 'pdf/linux.pdf' para Markdown...\n",
      "Convers√£o conclu√≠da.\n",
      "Encontrados 26 marcadores de cap√≠tulo.\n",
      "  üìÑ Cap√≠tulo 1: Cap√≠tulo 1\n",
      "  üìÑ Cap√≠tulo 2: Cap√≠tulo 2\n",
      "  üìÑ Cap√≠tulo 3: Cap√≠tulo 3\n",
      "  üìÑ Cap√≠tulo 4: Cap√≠tulo 4\n",
      "  üìÑ Cap√≠tulo 5: Cap√≠tulo 5\n",
      "  üìÑ Cap√≠tulo 6: Cap√≠tulo 6\n",
      "  üìÑ Cap√≠tulo 7: Cap√≠tulo 7\n",
      "  üìÑ Cap√≠tulo 8: Cap√≠tulo 8\n",
      "  üìÑ Cap√≠tulo 9: Cap√≠tulo 9\n",
      "  üìÑ Cap√≠tulo 10: Cap√≠tulo 10\n",
      "  üìÑ Cap√≠tulo 11: Cap√≠tulo 11\n",
      "  üìÑ Cap√≠tulo 12: Cap√≠tulo 12\n",
      "  üìÑ Cap√≠tulo 13: Cap√≠tulo 13\n",
      "  üìÑ Cap√≠tulo 14: Cap√≠tulo 14\n",
      "  üìÑ Cap√≠tulo 15: Cap√≠tulo 15\n",
      "  üìÑ Cap√≠tulo 16: Cap√≠tulo 16\n",
      "  üìÑ Cap√≠tulo 17: Cap√≠tulo 17\n",
      "  üìÑ Cap√≠tulo 18: Cap√≠tulo 18\n",
      "  üìÑ Cap√≠tulo 19: Cap√≠tulo 19\n",
      "  üìÑ Cap√≠tulo 20: Cap√≠tulo 20\n",
      "  üìÑ Cap√≠tulo 21: Cap√≠tulo 21\n",
      "  üìÑ Cap√≠tulo 22: Cap√≠tulo 22\n",
      "  üìÑ Cap√≠tulo 23: Cap√≠tulo 23\n",
      "  üìÑ Cap√≠tulo 24: Cap√≠tulo 24\n",
      "  üìÑ Cap√≠tulo 25: Cap√≠tulo 25\n",
      "  üìÑ Cap√≠tulo 26: Cap√≠tulo 26\n",
      "\n",
      "üöÄ Iniciando extra√ß√£o PARALELA em massa (26 cap√≠tulos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando em paralelo: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [03:26<00:00,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extra√ß√£o bruta conclu√≠da. Total: 360 conceitos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Pipeline de Extra√ß√£o em Massa (Paralelizado) ---\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "pdf_path = \"pdf/linux.pdf\"\n",
    "raw_concepts = [] # Lista para armazenar extra√ß√£o bruta\n",
    "\n",
    "# Fun√ß√£o auxiliar para processar um √∫nico cap√≠tulo (Isolamento da l√≥gica)\n",
    "def process_chapter_task(chapter):\n",
    "    try:\n",
    "        # Limitar contexto (aprox 40k caracteres para n√£o estourar tokens)\n",
    "        content_slice = chapter['content'][:40000]\n",
    "        \n",
    "        # Invocar gpt-4o-mini\n",
    "        result = chain_extractor.invoke({\n",
    "            \"chapter_id\": chapter['chapter_id'],\n",
    "            \"chapter_title\": chapter['title'],\n",
    "            \"content\": content_slice,\n",
    "            \"format_instructions\": parser.get_format_instructions()\n",
    "        })\n",
    "        \n",
    "        if result and result.conceitos:\n",
    "            # Retorna lista de dicts\n",
    "            return [c.model_dump() for c in result.conceitos]\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erro no cap√≠tulo {chapter['chapter_id']}: {str(e)}\")\n",
    "        return []\n",
    "    return []\n",
    "\n",
    "# --- Execu√ß√£o Principal ---\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"‚ùå Erro: Arquivo {pdf_path} n√£o encontrado.\")\n",
    "else:\n",
    "    # 1. Converter PDF (se necess√°rio)\n",
    "    if 'md_content' not in locals():\n",
    "        md_content = convert_pdf_to_markdown_memory(pdf_path)\n",
    "    \n",
    "    # 2. Segmentar Cap√≠tulos (se necess√°rio)\n",
    "    if 'chapters' not in locals() or not chapters:\n",
    "        chapters = split_markdown_chapters(md_content)\n",
    "    \n",
    "    # 3. Loop de Extra√ß√£o Paralela\n",
    "    if chapters:\n",
    "        print(f\"\\nüöÄ Iniciando extra√ß√£o PARALELA em massa ({len(chapters)} cap√≠tulos)...\")\n",
    "        \n",
    "        # max_workers=5 √© um bom equil√≠brio. Se tiver conta Tier 2+ na OpenAI, pode tentar 8 ou 10.\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            # Submete todas as tarefas\n",
    "            future_to_chapter = {executor.submit(process_chapter_task, ch): ch for ch in chapters}\n",
    "            \n",
    "            # Processa conforme v√£o ficando prontas (com barra de progresso)\n",
    "            for future in tqdm(as_completed(future_to_chapter), total=len(chapters), desc=\"Processando em paralelo\"):\n",
    "                result_concepts = future.result()\n",
    "                if result_concepts:\n",
    "                    raw_concepts.extend(result_concepts)\n",
    "\n",
    "        print(f\"\\nExtra√ß√£o bruta conclu√≠da. Total: {len(raw_concepts)} conceitos.\")\n",
    "    else:\n",
    "        print(\"Nenhum cap√≠tulo identificado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indu√ß√£o de Ontologia:** Utiliza um modelo LLM (GPT-4o) para analisar os tipos de conceitos extra√≠dos e criar dinamicamente uma taxonomia (ontologia) unificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Indu√ß√£o de Ontologia com GPT-4o...\n",
      "   Tipos brutos encontrados: 140\n",
      "Indu√ß√£o conclu√≠da.\n",
      "   Taxonomia criada: ['COMANDO', 'SISTEMA_ARQUIVOS', 'REDE', 'CONCEITO_TEORICO', 'FERRAMENTA', 'SHELL_SCRIPT', 'NOISE']\n"
     ]
    }
   ],
   "source": [
    "# --- Indu√ß√£o de Ontologia (GPT-4o) ---\n",
    "\n",
    "ontology_map = {}\n",
    "\n",
    "def induce_ontology(raw_data):\n",
    "    \"\"\"\n",
    "    Usa GPT-4o para criar uma taxonomia can√¥nica a partir dos tipos brutos.\n",
    "    (Substitu√≠do Claude 3.5 Sonnet por GPT-4o para evitar erro 404 e unificar provedor)\n",
    "    \"\"\"\n",
    "    print(\"Iniciando Indu√ß√£o de Ontologia com GPT-4o...\")\n",
    "    \n",
    "    # 1. Extrair tipos √∫nicos\n",
    "    unique_types = list(set([c['tipo'] for c in raw_data]))\n",
    "    print(f\"   Tipos brutos encontrados: {len(unique_types)}\")\n",
    "    \n",
    "    # Se houver muitos tipos, amostragem ou divis√£o em batches pode ser necess√°ria,\n",
    "    # mas para este escopo vamos enviar todos.\n",
    "    \n",
    "    try:\n",
    "        llm_ontologist = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        prompt_ontology = ChatPromptTemplate.from_messages([\n",
    "                    (\"system\", \"\"\"Voc√™ √© um Arquiteto de Ontologias S√™nior especializado em Grafos de Conhecimento Educacional (SINKT).\n",
    "                    \n",
    "                    Sua tarefa √© analisar uma lista de 'tipos de conceitos' extra√≠dos de forma bruta de um livro de Linux e criar uma Taxonomia Can√¥nica (Upper Ontology).\n",
    "                    \n",
    "                    DIRETRIZES ESTRITAS:\n",
    "                    1. Crie entre 6 a 12 categorias mestras (Can√¥nicas) que agrupem os conceitos t√©cnicos.\n",
    "                    Exemplos sugeridos: 'COMANDO', 'SISTEMA_ARQUIVOS', 'REDE', 'CONCEITO_TEORICO', 'FERRAMENTA', 'HARDWARE', 'PERMISSAO', 'SHELL_SCRIPT'.\n",
    "                    \n",
    "                    2. TRATAMENTO DE RU√çDO: Se um tipo bruto n√£o for um conceito t√©cnico ensin√°vel (ex: \"Metadado\", \"N√∫mero de P√°gina\", \"Autor\", \"√çndice\", \"Dica\"), mapeie-o para a categoria especial \"NOISE\".\n",
    "                    \n",
    "                    3. CONSIST√äNCIA: Comandos bin√°rios (ls, cd) devem ser 'COMANDO'. Arquivos e pastas (/etc, /bin) devem ser 'SISTEMA_ARQUIVOS'.\n",
    "                    \"\"\"),\n",
    "                    (\"user\", \"\"\"Analise a lista de tipos brutos abaixo e mapeie cada um para sua Categoria Can√¥nica.\n",
    "                    \n",
    "                    Tipos Brutos Encontrados:\n",
    "                    {raw_types}\n",
    "                    \n",
    "                    Retorne APENAS um JSON v√°lido no formato:\n",
    "                    {{\n",
    "                        \"map\": {{\n",
    "                            \"tipo_bruto_1\": \"COMANDO\",\n",
    "                            \"tipo_bruto_2\": \"NOISE\",\n",
    "                            \"tipo_bruto_3\": \"SISTEMA_ARQUIVOS\"\n",
    "                        }},\n",
    "                        \"taxonomy\": [\"COMANDO\", \"SISTEMA_ARQUIVOS\", \"REDE\", \"NOISE\", \"...\"]\n",
    "                    }}\n",
    "                    \"\"\")\n",
    "                ])\n",
    "        chain_ontologist = prompt_ontology | llm_ontologist | JsonOutputParser()\n",
    "        \n",
    "        result = chain_ontologist.invoke({\"raw_types\": json.dumps(unique_types, ensure_ascii=False)})\n",
    "        \n",
    "        print(\"Indu√ß√£o conclu√≠da.\")\n",
    "        print(f\"   Taxonomia criada: {result.get('taxonomy', [])}\")\n",
    "        return result.get('map', {})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Falha na Indu√ß√£o de Ontologia: {e}\")\n",
    "        print(\"   Usando fallback (mapa de identidade)...\")\n",
    "        return {t: t for t in unique_types}\n",
    "\n",
    "# Executar Indu√ß√£o se houver dados\n",
    "if raw_concepts:\n",
    "    ontology_map = induce_ontology(raw_concepts)\n",
    "else:\n",
    "    print(\"Sem dados brutos para induzir ontologia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normaliza√ß√£o e Consolida√ß√£o:** Padroniza os conceitos extra√≠dos utilizando a ontologia criada, removendo duplicatas e unificando defini√ß√µes e metadados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Normaliza√ß√£o e Consolida√ß√£o...\n",
      "   Itens removidos como 'NOISE': 41\n",
      "Processo Finalizado.\n",
      "   Conceitos √önicos Consolidados: 254\n"
     ]
    }
   ],
   "source": [
    "# --- Normaliza√ß√£o e Consolida√ß√£o Final ---\n",
    "\n",
    "def normalize_and_consolidate(raw_data, type_map):\n",
    "    print(\"Iniciando Normaliza√ß√£o e Consolida√ß√£o...\")\n",
    "    \n",
    "    # 1. Normalizar Tipos e Filtrar Ru√≠do\n",
    "    normalized_data = []\n",
    "    removed_count = 0\n",
    "    \n",
    "    for item in raw_data:\n",
    "        original_type = item['tipo']\n",
    "        # Usa o mapa, ou mant√©m original se n√£o encontrado\n",
    "        canonical_type = type_map.get(original_type, original_type)\n",
    "        \n",
    "        # FILTRAGEM DE RU√çDO: Se for mapeado como NOISE, descartamos.\n",
    "        if canonical_type == \"NOISE\":\n",
    "            removed_count += 1\n",
    "            continue\n",
    "\n",
    "        # Cria c√≥pia para n√£o mutar original (opcional, mas boa pr√°tica)\n",
    "        new_item = item.copy()\n",
    "        new_item['tipo'] = canonical_type\n",
    "        new_item['tipo_original'] = original_type # Guarda hist√≥rico\n",
    "        normalized_data.append(new_item)\n",
    "    \n",
    "    print(f\"   Itens removidos como 'NOISE': {removed_count}\")\n",
    "        \n",
    "    # 2. Consolidar (Deduplica√ß√£o por Nome)\n",
    "    consolidated = {}\n",
    "    \n",
    "    for concept in normalized_data:\n",
    "        key = concept['nome'].strip().lower()\n",
    "        \n",
    "        if key not in consolidated:\n",
    "            consolidated[key] = concept\n",
    "            consolidated[key]['_chapter_set'] = {concept['capitulo_origem']}\n",
    "        else:\n",
    "            existing = consolidated[key]\n",
    "            existing['_chapter_set'].add(concept['capitulo_origem'])\n",
    "            \n",
    "            # Resolu√ß√£o de conflitos de defini√ß√£o (mant√©m a mais longa)\n",
    "            if len(concept['definicao']) > len(existing['definicao']):\n",
    "                existing['definicao'] = concept['definicao']\n",
    "                existing['nome'] = concept['nome'] # Preserva casing do vencedor\n",
    "                \n",
    "            # Resolu√ß√£o de Tipo:\n",
    "            # Se j√° normalizamos, os tipos devem ser iguais.\n",
    "            # Se houver diverg√™ncia (ex: mesmo nome mapeado para categorias diferentes por contexto),\n",
    "            # Priorizamos por frequ√™ncia ou simplesmente mantemos o que j√° est√°.\n",
    "            # Aqui, como simplifica√ß√£o, mantemos o tipo do conceito com a melhor defini√ß√£o.\n",
    "            if len(concept['definicao']) > len(existing['definicao']):\n",
    "                 existing['tipo'] = concept['tipo']\n",
    "\n",
    "    # Finalizar\n",
    "    final_list = []\n",
    "    for item in consolidated.values():\n",
    "        chapters = sorted(list(item['_chapter_set']), key=lambda x: int(x) if x.isdigit() else 999)\n",
    "        item['capitulo_origem'] = \", \".join(chapters)\n",
    "        del item['_chapter_set']\n",
    "        \n",
    "        # Opcional: Remover campo auxiliar tipo_original para limpar JSON final\n",
    "        if 'tipo_original' in item:\n",
    "            del item['tipo_original']\n",
    "            \n",
    "        final_list.append(item)\n",
    "    \n",
    "    final_list.sort(key=lambda x: x['nome'].lower())\n",
    "    \n",
    "    print(f\"Processo Finalizado.\")\n",
    "    print(f\"   Conceitos √önicos Consolidados: {len(final_list)}\")\n",
    "    return final_list\n",
    "\n",
    "if raw_concepts and ontology_map:\n",
    "    final_concepts = normalize_and_consolidate(raw_concepts, ontology_map)\n",
    "else:\n",
    "    final_concepts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salvar Artefatos Finais:** Salva os dados processados (conceitos, ontologia e cap√≠tulos) em arquivos JSON para persist√™ncia e uso nas pr√≥ximas etapas do pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Conceitos salvos: output/01_extraction/concepts_map.json\n",
      "üíæ Mapa Ontol√≥gico salvo: output/01_extraction/ontology_map.json\n",
      "üíæ Conte√∫do dos cap√≠tulos salvo: output/01_extraction/chapters_content.json\n",
      "\n",
      "üëÅÔ∏è Exemplo (Conceito Normalizado):\n",
      "[\n",
      "  {\n",
      "    \"nome\": \"$4LINUX\",\n",
      "    \"tipo\": \"CONCEITO_TEORICO\",\n",
      "    \"definicao\": \"Utiliza√ß√£o do operador $ para acessar o valor armazenado na vari√°vel 4LINUX, permitindo que o conte√∫do da vari√°vel seja impresso ou utilizado em comandos.\",\n",
      "    \"capitulo_origem\": \"10\"\n",
      "  },\n",
      "  {\n",
      "    \"nome\": \"$PATH\",\n",
      "    \"tipo\": \"CONCEITO_TEORICO\",\n",
      "    \"definicao\": \"A vari√°vel de ambiente $PATH cont√©m uma lista de diret√≥rios onde o sistema busca execut√°veis. Permite que comandos sejam executados sem a necessidade de especificar o caminho absoluto.\",\n",
      "    \"capitulo_origem\": \"12\"\n",
      "  },\n",
      "  {\n",
      "    \"nome\": \"--help\",\n",
      "    \"tipo\": \"COMANDO\",\n",
      "    \"definicao\": \"O par√¢metro --help √© utilizado em comandos externos para fornecer uma consulta r√°pida sobre os par√¢metros e op√ß√µes dispon√≠veis para aquele comando espec√≠fico.\",\n",
      "    \"capitulo_origem\": \"16\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# --- Salvar Artefatos Finais ---\n",
    "\n",
    "output_folder = \"output/01_extraction\" # üìÇ NOVA PASTA\n",
    "output_ontology_path = f\"{output_folder}/ontology_map.json\"\n",
    "output_json_path = f\"{output_folder}/concepts_map.json\"\n",
    "output_chapters_path = f\"{output_folder}/chapters_content.json\"\n",
    "\n",
    "if final_concepts:\n",
    "    try:\n",
    "        # Criar pasta de sa√≠da\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Salvar Conceitos\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_concepts, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"üíæ Conceitos salvos: {output_json_path}\")\n",
    "        \n",
    "        # Salvar Mapa Ontol√≥gico\n",
    "        with open(output_ontology_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(ontology_map, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"üíæ Mapa Ontol√≥gico salvo: {output_ontology_path}\")\n",
    "\n",
    "        # Salvar Conte√∫do dos Cap√≠tulos (IMPORTANTE para a Parte 2)\n",
    "        if 'chapters' in locals() and chapters:\n",
    "            with open(output_chapters_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(chapters, f, indent=4, ensure_ascii=False)\n",
    "            print(f\"üíæ Conte√∫do dos cap√≠tulos salvo: {output_chapters_path}\")\n",
    "        \n",
    "        # Preview\n",
    "        print(\"\\nüëÅÔ∏è Exemplo (Conceito Normalizado):\")\n",
    "        print(json.dumps(final_concepts[:3], indent=2, ensure_ascii=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar arquivos: {e}\")\n",
    "else:\n",
    "    print(\"Nada a salvar.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
