{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 - The Council of Eight: Multi-Agent SINKT Densification\n",
    "\n",
    "Este notebook implementa a arquitetura **Multi-Agente (Swarm)** para densifica√ß√£o e valida√ß√£o do grafo de conhecimento SINKT. \n",
    "\n",
    "Substituindo as simula√ß√µes anteriores, utilizamos o **LangGraph** para orquestrar 8 agentes aut√¥nomos especializados, cada um utilizando um modelo de IA alocado estrategicamente de acordo com a complexidade da tarefa.\n",
    "\n",
    "## üèõÔ∏è O Conselho (The Council)\n",
    "\n",
    "| Agente | Fun√ß√£o | Modelo Alvo |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. The Scout** | Gera√ß√£o massiva de candidatos (Busca Vetorial). | `text-embedding-3-small` |\n",
    "| **2. The Bridge** | **Anti-Ilhas**: Conex√µes entre cap√≠tulos distantes. | `gpt-5.1` |\n",
    "| **3. The Professor** | Valida√ß√£o Pedag√≥gica (Pr√©-requisitos). | `gpt-5.1` |\n",
    "| **4. The Engineer** | Valida√ß√£o T√©cnica (Precis√£o Linux). | `claude-opus-4-5` |\n",
    "| **5. The Ontologist** | Classifica√ß√£o Sem√¢ntica (Taxonomia). | `gpt-4.1` |\n",
    "| **6. The Topologist** | Prote√ß√£o de DAG (Detec√ß√£o de Ciclos). | `gpt-4.1` + *Python Tools* |\n",
    "| **7. The Cleaner** | Remo√ß√£o de Ru√≠do e Alucina√ß√µes. | `gpt-4.1-mini` |\n",
    "| **8. The Judge** | Veredito Final e Resolu√ß√£o de Conflitos. | `gpt-5.1` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelos Configurados e Carregados (Conselho de 4 Agentes).\n"
     ]
    }
   ],
   "source": [
    "# Instala√ß√£o do LangGraph e depend√™ncias\n",
    "!pip install -q langgraph langchain langchain-openai langchain-anthropic networkx scikit-learn numpy pydantic tqdm python-dotenv\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from typing import List, Literal, Optional, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- CONFIGURA√á√ÉO DE MODELOS (TIERED COMPUTE) ---\n",
    "def get_model(model_name: str, temperature: float = 0):\n",
    "    # Fallback seguro para modelos existentes\n",
    "    if \"claude\" in model_name:\n",
    "        return ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=temperature)\n",
    "    elif \"gpt-4o-mini\" in model_name:\n",
    "         return ChatOpenAI(model=\"gpt-4o-mini\", temperature=temperature)\n",
    "    else:\n",
    "        # Padr√£o robusto para tarefas complexas\n",
    "        return ChatOpenAI(model=\"gpt-4o\", temperature=temperature)\n",
    "\n",
    "# Registro de Modelos do Conselho (4 Agentes Otimizados)\n",
    "MODELS = {\n",
    "    \"scout_embed\": OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    \"cleaner\": get_model(\"gpt-4o-mini\"),  # Triagem R√°pida e Barata\n",
    "    \"expert\": get_model(\"gpt-4o\"),        # Valida√ß√£o T√©cnica e Pedag√≥gica Profunda\n",
    "    \"analyst\": get_model(\"gpt-4o\"),       # Estrutura e Ontologia\n",
    "    \"judge\": get_model(\"gpt-4o\"),         # S√≠ntese e Decis√£o\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Modelos Configurados e Carregados (Conselho de 4 Agentes).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dados Carregados: 216 conceitos | 207 rela√ß√µes iniciais.\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP DE DIRET√ìRIOS ---\n",
    "INPUT_FOLDER = \"output/01_extraction\"\n",
    "OUTPUT_FOLDER = \"output/03_council_execution\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "CONCEPTS_FILE = f\"{INPUT_FOLDER}/concepts_map.json\"\n",
    "RELATIONS_FILE = f\"{INPUT_FOLDER}/relations_initial.json\"\n",
    "\n",
    "# Carregar Dados Iniciais\n",
    "def load_data():\n",
    "    with open(CONCEPTS_FILE, 'r', encoding='utf-8') as f:\n",
    "        concepts = json.load(f)\n",
    "    with open(RELATIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "        relations = json.load(f)\n",
    "    return concepts, relations\n",
    "\n",
    "concepts, initial_relations = load_data()\n",
    "\n",
    "# Remover ru√≠dos pr√©vios (se houver)\n",
    "concepts = [c for c in concepts if c.get('tipo') != 'NOISE']\n",
    "\n",
    "print(f\"üìä Dados Carregados: {len(concepts)} conceitos | {len(initial_relations)} rela√ß√µes iniciais.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fase de Gera√ß√£o de Candidatos (Scout & Bridge)\n",
    "\n",
    "Aqui combinamos duas estrat√©gias:\n",
    "1.  **Scout:** Similaridade de Cosseno com threshold relaxado (0.75) para capturar muitas possibilidades.\n",
    "2.  **Bridge:** L√≥gica expl√≠cita para for√ßar compara√ß√µes entre conceitos de cap√≠tulos distantes (evitar ilhas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üïµÔ∏è Scout & Bridge iniciando varredura...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Candidatos Gerados: 252\n",
      "   - Via Bridge (Anti-Ilha): 150 (de 156 encontrados)\n",
      "   - Via Scout (Similaridade): 102 (de 102 encontrados)\n"
     ]
    }
   ],
   "source": [
    "def generate_candidates(concepts, relations_existing):\n",
    "    print(\"\\nüïµÔ∏è Scout & Bridge iniciando varredura...\")\n",
    "    \n",
    "    # 1. Preparar Grafo Existente (para n√£o sugerir duplicatas)\n",
    "    G_exist = nx.DiGraph()\n",
    "    for r in relations_existing:\n",
    "        G_exist.add_edge(r['source'], r['target'])\n",
    "        \n",
    "    # 2. Gerar Embeddings\n",
    "    texts = [f\"{c['nome']} ({c['tipo']}): {c['definicao']} [Cap {c['capitulo_origem']}]\" for c in concepts]\n",
    "    vectors = MODELS['scout_embed'].embed_documents(texts)\n",
    "    matrix = np.array(vectors)\n",
    "    sim_matrix = cosine_similarity(matrix)\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # Par√¢metros Scout\n",
    "    THRESHOLD_SCOUT = 0.75\n",
    "    \n",
    "    # Par√¢metros Bridge (Anti-Ilha)\n",
    "    # Se a dist√¢ncia entre cap√≠tulos for > 3, aceitamos um threshold menor para encorajar conex√µes longas\n",
    "    THRESHOLD_BRIDGE = 0.70\n",
    "    \n",
    "    count_scout = 0\n",
    "    count_bridge = 0\n",
    "    \n",
    "    for i in range(len(concepts)):\n",
    "        source = concepts[i]\n",
    "        # Extrair primeiro cap√≠tulo como refer√™ncia (simplifica√ß√£o)\n",
    "        src_chap = int(source['capitulo_origem'].split(',')[0]) if source['capitulo_origem'][0].isdigit() else 0\n",
    "        \n",
    "        for j in range(len(concepts)):\n",
    "            if i == j: continue\n",
    "            \n",
    "            target = concepts[j]\n",
    "            tgt_chap = int(target['capitulo_origem'].split(',')[0]) if target['capitulo_origem'][0].isdigit() else 0\n",
    "            \n",
    "            # Verificar se j√° existe\n",
    "            if G_exist.has_edge(source['nome'], target['nome']) or G_exist.has_edge(target['nome'], source['nome']):\n",
    "                continue\n",
    "                \n",
    "            score = sim_matrix[i][j]\n",
    "            chap_dist = abs(src_chap - tgt_chap)\n",
    "            \n",
    "            # L√≥gica de Sele√ß√£o\n",
    "            category = None\n",
    "            \n",
    "            # BRIDGE: Dist√¢ncia alta e score razo√°vel\n",
    "            if chap_dist >= 3 and score > THRESHOLD_BRIDGE:\n",
    "                category = 'BRIDGE_CANDIDATE'\n",
    "                count_bridge += 1\n",
    "            \n",
    "            # SCOUT: Score alto independente da dist√¢ncia\n",
    "            elif score > THRESHOLD_SCOUT:\n",
    "                category = 'SCOUT_CANDIDATE'\n",
    "                count_scout += 1\n",
    "                \n",
    "            if category:\n",
    "                candidates.append({\n",
    "                    \"source\": source['nome'],\n",
    "                    \"target\": target['nome'],\n",
    "                    \"source_type\": source['tipo'],\n",
    "                    \"target_type\": target['tipo'],\n",
    "                    \"score\": float(score),\n",
    "                    \"origin\": category,\n",
    "                    \"chapter_dist\": chap_dist\n",
    "                })\n",
    "                \n",
    "    # Deduplicar (A->B e B->A podem ter scores id√™nticos, manter apenas um par para o Conselho decidir a dire√ß√£o)\n",
    "    # Mas aqui deixaremos ambos pois a dire√ß√£o importa para o embeddings as vezes? N√£o, cosseno √© sim√©trico.\n",
    "    # Vamos filtrar para n√£o superlotar o conselho: Top 300 candidatos mais fortes + Top 100 Bridges\n",
    "    \n",
    "    bridges = sorted([c for c in candidates if c['origin'] == 'BRIDGE_CANDIDATE'], key=lambda x: x['score'], reverse=True)[:150]\n",
    "    scouts = sorted([c for c in candidates if c['origin'] == 'SCOUT_CANDIDATE'], key=lambda x: x['score'], reverse=True)[:350]\n",
    "    \n",
    "    final_candidates = bridges + scouts\n",
    "    \n",
    "    print(f\"‚úÖ Candidatos Gerados: {len(final_candidates)}\")\n",
    "    print(f\"   - Via Bridge (Anti-Ilha): {len(bridges)} (de {count_bridge} encontrados)\")\n",
    "    print(f\"   - Via Scout (Similaridade): {len(scouts)} (de {count_scout} encontrados)\")\n",
    "    \n",
    "    return final_candidates\n",
    "\n",
    "# Executar Gera√ß√£o\n",
    "candidates = generate_candidates(concepts, initial_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defini√ß√£o do LangGraph (O Conselho)\n",
    "\n",
    "Definimos o `GraphState` que carrega o dossi√™ da aresta e os n√≥s para cada agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ESTADO DO GRAFO ---\n",
    "class AgentVote(BaseModel):\n",
    "    agent: str\n",
    "    verdict: Literal['APPROVE', 'REJECT', 'MODIFY', 'ABSTAIN']\n",
    "    suggested_type: Optional[str] = None\n",
    "    reason: str\n",
    "\n",
    "class EdgeState(BaseModel):\n",
    "    source: str\n",
    "    target: str\n",
    "    source_type: str\n",
    "    target_type: str\n",
    "    current_type: Optional[str] = \"RELATED_TO\" # Tipo proposto inicialmente\n",
    "    similarity_score: float\n",
    "    origin: str # Bridge ou Scout\n",
    "    \n",
    "    # Votos dos Agentes\n",
    "    votes: List[AgentVote] = []\n",
    "    \n",
    "    # Decis√£o Final\n",
    "    final_verdict: Optional[Literal['KEEP', 'DISCARD']] = None\n",
    "    final_type: Optional[str] = None\n",
    "    final_reason: Optional[str] = None\n",
    "\n",
    "# --- AGENTES OTIMIZADOS --- \n",
    "\n",
    "# 1. THE CLEANER (Triagem R√°pida - GPT-4o-Mini)\n",
    "def agent_cleaner(state: EdgeState):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Voc√™ √© o CLEANER do grafo de conhecimento. Sua fun√ß√£o √© proteger o sistema de lixo e alucina√ß√µes.\n",
    "        \n",
    "        Analise a aresta proposta:\n",
    "        {source} ({source_type}) -> {target} ({target_type})\n",
    "        Score Vetorial: {score}\n",
    "        \n",
    "        ### CRIT√âRIOS DE ELIMINA√á√ÉO IMEDIATA (Vote REJECT):\n",
    "        1. **Alucina√ß√µes:** Conceitos que n√£o existem no contexto Linux (ex: \"P√°gina 12\", \"Cap√≠tulo 4\", \"Jos√© Silva\").\n",
    "        2. **Tipos Errados:** Vari√°veis soltas (ex: \"$PATH\") conectadas a conceitos te√≥ricos sem sentido.\n",
    "        3. **Meta-dados:** Conex√µes com artefatos do livro (ex: \"Figura 1.1\", \"Tabela 2\").\n",
    "        \n",
    "        Se a conex√£o for minimamente plaus√≠vel tecnicamente, vote ABSTAIN para deixar os especialistas decidirem.\n",
    "        Seja conservador na aprova√ß√£o, mas agressivo na elimina√ß√£o de lixo √≥bvio.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    response = MODELS['cleaner'].invoke(prompt.format(\n",
    "        source=state.source, source_type=state.source_type,\n",
    "        target=state.target, target_type=state.target_type,\n",
    "        score=state.similarity_score\n",
    "    ))\n",
    "    \n",
    "    verdict = 'REJECT' if 'REJECT' in response.content.upper() else 'ABSTAIN'\n",
    "    state.votes.append(AgentVote(agent=\"Cleaner\", verdict=verdict, reason=response.content[:150]))\n",
    "    return state\n",
    "\n",
    "# 2. THE EXPERT (T√©cnico & Pedagogo - GPT-4o)\n",
    "def agent_expert(state: EdgeState):\n",
    "    # Se Cleaner rejeitou, n√£o gasta token caro\n",
    "    if any(v.verdict == 'REJECT' for v in state.votes): return state\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Voc√™ √© o ESPECIALISTA S√äNIOR (Engenheiro Linux + Pedagogo).\n",
    "        Analise a rela√ß√£o: {source} -> {target}.\n",
    "        \n",
    "        ### SUAS DUAS MISS√ïES:\n",
    "        1. **Valida√ß√£o T√©cnica (Engineer):** A rela√ß√£o √© tecnicamente verdadeira?\n",
    "           - Ex: 'ls' realmente lista arquivos? SIM.\n",
    "           - Ex: 'Kernel' √© um tipo de 'Mouse'? N√ÉO (REJECT).\n",
    "           \n",
    "        2. **Valida√ß√£o Pedag√≥gica (Professor):** Existe depend√™ncia de aprendizado?\n",
    "           - Se eu preciso aprender A para entender B -> Vote MODIFY e sugira **PREREQUISITE**.\n",
    "           - Se A √© uma ferramenta usada por B -> Vote MODIFY e sugira **USE**.\n",
    "           - Se A comp√µe B -> Vote MODIFY e sugira **PART_OF**.\n",
    "           - Se √© apenas relacionado -> Vote APPROVE (RELATED_TO).\n",
    "           \n",
    "        Responda com veredito, tipo sugerido e raz√£o breve.\n",
    "        \"\"\"\n",
    "    )\n",
    "    # CORRE√á√ÉO: Usar .format() antes de invocar o modelo\n",
    "    response = MODELS['expert'].invoke(prompt.format(source=state.source, target=state.target))\n",
    "    \n",
    "    content = response.content.upper()\n",
    "    verdict = 'APPROVE'\n",
    "    suggestion = None\n",
    "    \n",
    "    if 'REJECT' in content:\n",
    "        verdict = 'REJECT'\n",
    "    elif 'PREREQUISITE' in content:\n",
    "        verdict = 'MODIFY'\n",
    "        suggestion = 'PREREQUISITE'\n",
    "    elif 'USE' in content:\n",
    "        verdict = 'MODIFY'\n",
    "        suggestion = 'USE'\n",
    "    elif 'PART_OF' in content:\n",
    "        verdict = 'MODIFY'\n",
    "        suggestion = 'PART_OF'\n",
    "    \n",
    "    state.votes.append(AgentVote(agent=\"Expert\", verdict=verdict, suggested_type=suggestion, reason=response.content[:200]))\n",
    "    return state\n",
    "\n",
    "# 3. THE ANALYST (Estrutura & Ontologia - GPT-4o)\n",
    "def agent_analyst(state: EdgeState):\n",
    "    if any(v.verdict == 'REJECT' for v in state.votes): return state\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Voc√™ √© o ANALISTA ESTRUTURAL (Top√≥logo + Ontologista).\n",
    "        Analise: {source} ({source_type}) -> {target} ({target_type}).\n",
    "        \n",
    "        ### CHECAGEM DE CONSIST√äNCIA:\n",
    "        1. **Hierarquia de Tipos:**\n",
    "           - Um 'Conceito Abstrato' n√£o pode ser PART_OF um 'Comando'.\n",
    "           - Hierarquia deve fluir do Geral -> Espec√≠fico ou Componente -> Todo.\n",
    "           \n",
    "        2. **Topologia (Preven√ß√£o de Ciclos Locais):**\n",
    "           - Verifique se a dire√ß√£o da seta faz sentido l√≥gico.\n",
    "           - Se estiver invertida (ex: 'Linux' PART_OF 'Kernel'), vote REJECT ou sugira invers√£o no coment√°rio.\n",
    "        \n",
    "        Se tudo estiver logicamente consistente, vote APPROVE. Se houver erro categ√≥rico, vote REJECT.\n",
    "        \"\"\"\n",
    "    )\n",
    "    # CORRE√á√ÉO: Usar .format() antes de invocar o modelo\n",
    "    response = MODELS['analyst'].invoke(prompt.format(\n",
    "        source=state.source, source_type=state.source_type,\n",
    "        target=state.target, target_type=state.target_type\n",
    "    ))\n",
    "    \n",
    "    verdict = 'REJECT' if 'REJECT' in response.content.upper() else 'APPROVE'\n",
    "    state.votes.append(AgentVote(agent=\"Analyst\", verdict=verdict, reason=response.content[:150]))\n",
    "    return state\n",
    "\n",
    "# 4. THE JUDGE (Decisor Final - GPT-4o)\n",
    "class JudgeVerdict(BaseModel):\n",
    "    final_action: Literal['KEEP', 'DISCARD']\n",
    "    final_type: str\n",
    "    rationale: str\n",
    "\n",
    "def agent_judge(state: EdgeState):\n",
    "    # Formata dossi√™ de votos\n",
    "    votes_str = \"\\n\".join([f\"- {v.agent}: {v.verdict} ({v.suggested_type or 'N/A'}) -> {v.reason}\" for v in state.votes])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Voc√™ √© o JUIZ SUPREMO do grafo SINKT. Sua palavra √© final.\"),\n",
    "        (\"user\", \"\"\"Decida o destino desta aresta: {source} -> {target}\n",
    "        \n",
    "        VOTOS DO CONSELHO:\n",
    "        {votes}\n",
    "        \n",
    "        ### REGRAS DE JULGAMENTO:\n",
    "        1. **Veto T√©cnico:** Se o EXPERT ou CLEANER rejeitou, o veredito √© DISCARD.\n",
    "        2. **Tipifica√ß√£o:** Priorize o tipo sugerido pelo EXPERT (ex: PREREQUISITE) sobre tipos gen√©ricos.\n",
    "        3. **Seguran√ßa:** Na d√∫vida entre APPROVE e REJECT, prefira DISCARD para manter o grafo limpo.\n",
    "        \n",
    "        Retorne JSON compat√≠vel.\n",
    "        {format_instructions}\n",
    "        \"\"\")\n",
    "    ])\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=JudgeVerdict)\n",
    "    try:\n",
    "        # CORRE√á√ÉO CR√çTICA: Invocar o modelo passando as mensagens formatadas pelo prompt, n√£o um dicion√°rio cru.\n",
    "        # O dicion√°rio vai para o .format(), e o resultado do .format() vai para o .invoke()\n",
    "        \n",
    "        formatted_messages = prompt.format_messages(\n",
    "            source=state.source, \n",
    "            target=state.target, \n",
    "            votes=votes_str,\n",
    "            format_instructions=parser.get_format_instructions()\n",
    "        )\n",
    "        \n",
    "        response = MODELS['judge'].invoke(formatted_messages)\n",
    "        verdict = parser.parse(response.content)\n",
    "        \n",
    "        state.final_verdict = verdict.final_action\n",
    "        state.final_type = verdict.final_type\n",
    "        state.final_reason = verdict.rationale\n",
    "    except Exception as e:\n",
    "        state.final_verdict = \"DISCARD\"\n",
    "        state.final_reason = f\"Erro Julgamento: {e}\"\n",
    "        # Adicionar print para debug no console tamb√©m\n",
    "        print(f\"CRITICAL JUDGE ERROR: {e}\")\n",
    "        \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Montagem do Workflow (LangGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Workflow do Conselho (4 Agentes) compilado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Configurar Grafo (Otimizado 4 Agentes)\n",
    "workflow = StateGraph(EdgeState)\n",
    "\n",
    "# Adicionar N√≥s\n",
    "workflow.add_node(\"cleaner\", agent_cleaner)\n",
    "workflow.add_node(\"expert\", agent_expert)\n",
    "workflow.add_node(\"analyst\", agent_analyst)\n",
    "workflow.add_node(\"judge\", agent_judge)\n",
    "\n",
    "# Definir Arestas (Fluxo)\n",
    "workflow.set_entry_point(\"cleaner\")\n",
    "\n",
    "def cleaner_router(state):\n",
    "    # Se cleaner rejeitar, vai direto pro fim (Judge apenas carimba)\n",
    "    # Isso economiza execu√ß√£o dos agentes caros (Expert/Analyst)\n",
    "    if any(v.verdict == 'REJECT' and v.agent == 'Cleaner' for v in state.votes):\n",
    "        return \"judge\"\n",
    "    return \"expert\"\n",
    "\n",
    "workflow.add_conditional_edges(\"cleaner\", cleaner_router)\n",
    "\n",
    "# Fluxo Linear: Expert -> Analyst -> Judge\n",
    "workflow.add_edge(\"expert\", \"analyst\")\n",
    "workflow.add_edge(\"analyst\", \"judge\")\n",
    "workflow.add_edge(\"judge\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "print(\"‚úÖ Workflow do Conselho (4 Agentes) compilado com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Iniciando Sess√£o do Conselho para 252 candidatos...\n",
      "üìå Restam 252 candidatos para processar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 252/252 [43:57<00:00, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Processo Conclu√≠do.\n",
      "Arestas Aprovadas Totais: 53\n",
      "üìù Log detalhado salvo em: output/03_council_execution/council_execution.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- EXECU√á√ÉO EM LOTE COM CHECKPOINT E LOGGING ---\n",
    "\n",
    "print(f\"‚öñÔ∏è Iniciando Sess√£o do Conselho para {len(candidates)} candidatos...\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "CHECKPOINT_FILE = f\"{OUTPUT_FOLDER}/checkpoint_edges.json\"\n",
    "LOG_FILE = f\"{OUTPUT_FOLDER}/council_execution.log\"\n",
    "final_edges = []\n",
    "processed_pairs = set()\n",
    "\n",
    "# 1. Tentar carregar checkpoint existente\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:\n",
    "            saved_data = json.load(f)\n",
    "            final_edges = saved_data.get('edges', [])\n",
    "            processed_pairs_all = set(saved_data.get('processed_pairs', []))\n",
    "            \n",
    "        print(f\"üîÑ Checkpoint carregado! {len(final_edges)} arestas aprovadas recuperadas.\")\n",
    "        print(f\"‚è© Pulando {len(processed_pairs_all)} pares j√° processados anteriormente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro ao ler checkpoint: {e}. Iniciando do zero.\")\n",
    "        processed_pairs_all = set()\n",
    "else:\n",
    "    processed_pairs_all = set()\n",
    "\n",
    "# Fun√ß√£o Helper para Log\n",
    "def log_execution(pair, votes, verdict, reason):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"\\n{'='*50}\\n\")\n",
    "        log.write(f\"[{timestamp}] PAR: {pair}\\n\")\n",
    "        log.write(f\"VEREDITO FINAL: {verdict}\\n\")\n",
    "        log.write(f\"RAZ√ÉO: {reason}\\n\")\n",
    "        log.write(\"-\" * 20 + \" VOTOS \" + \"-\" * 20 + \"\\n\")\n",
    "        for v in votes:\n",
    "            log.write(f\" > {v.agent}: {v.verdict} (Sugest√£o: {v.suggested_type or 'N/A'}) | {v.reason[:100]}...\\n\")\n",
    "\n",
    "# Processar em batch\n",
    "candidates_to_process = [c for c in candidates if f\"{c['source']}|{c['target']}\" not in processed_pairs_all]\n",
    "\n",
    "print(f\"üìå Restam {len(candidates_to_process)} candidatos para processar.\")\n",
    "\n",
    "# Inicializar cabe√ßalho do log se arquivo novo\n",
    "if not os.path.exists(LOG_FILE):\n",
    "    with open(LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"--- LOG DE EXECU√á√ÉO DO CONSELHO ---\\n\")\n",
    "\n",
    "for i, cand in enumerate(tqdm(candidates_to_process)):\n",
    "    pair_id = f\"{cand['source']}|{cand['target']}\"\n",
    "    \n",
    "    initial_state = EdgeState(\n",
    "        source=cand['source'],\n",
    "        target=cand['target'],\n",
    "        source_type=cand['source_type'],\n",
    "        target_type=cand['target_type'],\n",
    "        similarity_score=cand['score'],\n",
    "        origin=cand['origin']\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = app.invoke(initial_state)\n",
    "        \n",
    "        # Logar execu√ß√£o detalhada\n",
    "        log_execution(\n",
    "            pair=pair_id, \n",
    "            votes=result['votes'], \n",
    "            verdict=result['final_verdict'], \n",
    "            reason=result['final_reason']\n",
    "        )\n",
    "        \n",
    "        # Se aprovado, adiciona aos finais\n",
    "        if result['final_verdict'] == 'KEEP':\n",
    "            final_edges.append({\n",
    "                \"source\": result['source'],\n",
    "                \"target\": result['target'],\n",
    "                \"type\": result['final_type'],\n",
    "                \"reason\": result['final_reason'],\n",
    "                \"origin\": result['origin']\n",
    "            })\n",
    "            \n",
    "        processed_pairs_all.add(pair_id)\n",
    "        \n",
    "        # SALVAMENTO IMEDIATO (A CADA RETORNO)\n",
    "        with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"edges\": final_edges,\n",
    "                \"processed_pairs\": list(processed_pairs_all)\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no par {pair_id}: {e}\")\n",
    "        # Logar erro\n",
    "        with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log:\n",
    "             log.write(f\"\\n‚ùå ERRO NO PROCESSAMENTO DE {pair_id}: {e}\\n\")\n",
    "        pass\n",
    "\n",
    "print(f\"\\n‚úÖ Processo Conclu√≠do.\")\n",
    "print(f\"Arestas Aprovadas Totais: {len(final_edges)}\")\n",
    "print(f\"üìù Log detalhado salvo em: {LOG_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "RELAT√ìRIO FINAL (MULTI-AGENT SWARM - OTIMIZADO)\n",
      "========================================\n",
      "N√≥s: 216\n",
      "Arestas: 260\n",
      "Densidade: 0.00560\n",
      "üíæ Grafo Salvo: output/03_council_execution/final_sinkt_graph_swarm.json\n"
     ]
    }
   ],
   "source": [
    "# --- CONSOLIDA√á√ÉO FINAL E SALVAMENTO ---\n",
    "\n",
    "# Unir com as arestas originais (Assumimos que as originais da fase 2 s√£o 'Ground Truth' parciais,\n",
    "# mas idealmente deveriam passar pelo crivo tamb√©m. Por hora, mantemos e adicionamos as novas)\n",
    "\n",
    "all_relations = initial_relations + final_edges\n",
    "\n",
    "# Construir Grafo Final para M√©tricas\n",
    "G_final = nx.DiGraph()\n",
    "for c in concepts:\n",
    "    G_final.add_node(c['nome'], tipo=c['tipo'])\n",
    "\n",
    "for r in all_relations:\n",
    "    if G_final.has_node(r['source']) and G_final.has_node(r['target']):\n",
    "        G_final.add_edge(r['source'], r['target'], type=r['type'])\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"RELAT√ìRIO FINAL (MULTI-AGENT SWARM - OTIMIZADO)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"N√≥s: {G_final.number_of_nodes()}\")\n",
    "print(f\"Arestas: {G_final.number_of_edges()}\")\n",
    "print(f\"Densidade: {nx.density(G_final):.5f}\")\n",
    "\n",
    "# Salvar\n",
    "output_data = {\n",
    "    \"metadata\": {\n",
    "        \"method\": \"Multi-Agent Council V2 (LangGraph Optimized)\",\n",
    "        \"agents\": [\"Cleaner\", \"Expert\", \"Analyst\", \"Judge\"],\n",
    "        \"models_used\": list(MODELS.keys())\n",
    "    },\n",
    "    \"concepts\": concepts,\n",
    "    \"relations\": all_relations\n",
    "}\n",
    "\n",
    "FINAL_FILE = f\"{OUTPUT_FOLDER}/final_sinkt_graph_swarm.json\"\n",
    "with open(FINAL_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "print(f\"üíæ Grafo Salvo: {FINAL_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
