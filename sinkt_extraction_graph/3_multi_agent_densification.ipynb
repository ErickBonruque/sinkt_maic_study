{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 3 - Densifica√ß√£o H√≠brida (Vetorial + Agentes)\n",
    "\n",
    "Nesta etapa, utilizamos uma abordagem agressiva, por√©m controlada, para densificar o grafo de conhecimento.\n",
    "\n",
    "**Diagn√≥stico:** A densidade inicial (~0.004) √© insuficiente para algoritmos de Knowledge Tracing (SINKT), que dependem de caminhos conectados para propagar infer√™ncia.\n",
    "\n",
    "**Pipeline Atualizado:**\n",
    "1.  **The Cleaner (Faxineiro):** Remove ru√≠dos de extra√ß√£o antes do processamento caro.\n",
    "2.  **The Architect (H√≠brido):**\n",
    "    *   **Matem√°tica (Scout):** Gera embeddings (text-embedding-3-small) e usa similaridade de cosseno com threshold r√≠gido (0.89) para encontrar candidatos a conex√£o.\n",
    "    *   **LLM (Validator):** Valida semanticamente os candidatos matem√°ticos e define o tipo de aresta (USE, RELATED_TO).\n",
    "3.  **The Teacher (Pedagogo):** Analisa as novas conex√µes e promove para PREREQUISITE quando h√° depend√™ncia de aprendizado.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-openai networkx scikit-learn numpy pydantic tqdm python-dotenv\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from typing import List, Literal, Tuple\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configura√ß√£o de Modelos\n",
    "llm_mini = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# --- NOVO DIRET√ìRIO DE ENTRADA/SA√çDA ---\n",
    "INPUT_FOLDER = \"output/01_extraction\"\n",
    "OUTPUT_FOLDER = \"output/02_densification\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "CONCEPTS_FILE = f\"{INPUT_FOLDER}/concepts_map.json\"\n",
    "RELATIONS_FILE = f\"{INPUT_FOLDER}/relations_initial.json\" # Atualizado para ler o novo output da fase 2\n",
    "ENHANCED_FILE = f\"{OUTPUT_FOLDER}/enhanced_graph.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado Inicial:\n",
      "Nodes: 230 | Edges: 211\n",
      "Densidade: 0.00585\n"
     ]
    }
   ],
   "source": [
    "## 1. Carregamento e Estado Inicial\n",
    "\n",
    "def load_state():\n",
    "    with open(CONCEPTS_FILE, 'r', encoding='utf-8') as f:\n",
    "        concepts = json.load(f)\n",
    "    with open(RELATIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "        relations = json.load(f)\n",
    "    return concepts, relations\n",
    "\n",
    "concepts, relations = load_state()\n",
    "\n",
    "# Construir Grafo Inicial para verifica√ß√£o de exist√™ncia\n",
    "G_initial = nx.DiGraph()\n",
    "for r in relations:\n",
    "    G_initial.add_edge(r['source'], r['target'])\n",
    "\n",
    "print(f\"Estado Inicial:\")\n",
    "print(f\"Nodes: {len(concepts)} | Edges: {len(relations)}\")\n",
    "print(f\"Densidade: {nx.density(G_initial):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agente de Limpeza (Cleaner):** Executa uma varredura inicial para remover conceitos que s√£o claramente erros de extra√ß√£o ou irrelevantes, economizando tokens e processamento nas etapas seguintes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ Cleaner trabalhando...\n",
      "   Batch 1: 17 ru√≠dos encontrados.\n",
      "   Batch 2: 9 ru√≠dos encontrados.\n",
      "Conceitos V√°lidos: 204 (Ru√≠dos removidos: 26)\n"
     ]
    }
   ],
   "source": [
    "## 2. Agente 1: The Cleaner (Faxineiro)\n",
    "# Removemos ru√≠dos antes de gerar embeddings para n√£o sujar o espa√ßo vetorial.\n",
    "\n",
    "class NoiseConcept(BaseModel):\n",
    "    nome: str = Field(description=\"Nome do conceito ru√≠do\")\n",
    "    razao: str\n",
    "\n",
    "class ConceptReview(BaseModel):\n",
    "    noise_concepts: List[NoiseConcept]\n",
    "\n",
    "cleaner_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Identifique ru√≠dos na lista de conceitos t√©cnicos de Linux.\n",
    "        RU√çDOS: Vari√°veis soltas ($VAR), erros de OCR, n√∫meros, termos do livro (Cap√≠tulo, P√°gina), verbos soltos.\n",
    "        MANTENHA: Comandos, caminhos (/etc), siglas, conceitos te√≥ricos.\"\"\"),\n",
    "        (\"user\", \"Lista: {concepts}\\n{format_instructions}\")\n",
    "    ])\n",
    "    | llm_mini\n",
    "    | PydanticOutputParser(pydantic_object=ConceptReview)\n",
    ")\n",
    "\n",
    "def run_cleaner(all_concepts):\n",
    "    names = [c['nome'] for c in all_concepts]\n",
    "    noise_found = []\n",
    "    batch_size = 150\n",
    "    \n",
    "    print(\"\\nüßπ Cleaner trabalhando...\")\n",
    "    for i in range(0, len(names), batch_size):\n",
    "        batch = names[i:i+batch_size]\n",
    "        try:\n",
    "            res = cleaner_chain.invoke({\n",
    "                \"concepts\": json.dumps(batch, ensure_ascii=False),\n",
    "                \"format_instructions\": PydanticOutputParser(pydantic_object=ConceptReview).get_format_instructions()\n",
    "            })\n",
    "            if res.noise_concepts:\n",
    "                noise_found.extend([n.nome for n in res.noise_concepts])\n",
    "                print(f\"   Batch {i//batch_size + 1}: {len(res.noise_concepts)} ru√≠dos encontrados.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro Cleaner: {e}\")\n",
    "            \n",
    "    return set(noise_found)\n",
    "\n",
    "noise_set = run_cleaner(concepts)\n",
    "clean_concepts = [c for c in concepts if c['nome'] not in noise_set]\n",
    "print(f\"Conceitos V√°lidos: {len(clean_concepts)} (Ru√≠dos removidos: {len(noise_set)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gera√ß√£o de Embeddings:** Cria representa√ß√µes vetoriais ricas para cada conceito, combinando nome, tipo e defini√ß√£o. Isso permite encontrar similaridades sem√¢nticas profundas que n√£o seriam detectadas apenas por compara√ß√£o de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß¨ Gerando Embeddings (text-embedding-3-small)...\n",
      "Embeddings gerados. Shape: (204, 1536)\n"
     ]
    }
   ],
   "source": [
    "## 3. Prepara√ß√£o Vetorial (Embeddings)\n",
    "\n",
    "print(\"\\nüß¨ Gerando Embeddings (text-embedding-3-small)...\")\n",
    "\n",
    "# Criar representa√ß√£o rica para o embedding: \"Nome (Tipo): Defini√ß√£o\"\n",
    "texts_to_embed = [f\"{c['nome']} ({c['tipo']}): {c['definicao']}\" for c in clean_concepts]\n",
    "\n",
    "try:\n",
    "    vectors = embeddings_model.embed_documents(texts_to_embed)\n",
    "    vectors_np = np.array(vectors)\n",
    "    print(f\"Embeddings gerados. Shape: {vectors_np.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar embeddings: {e}\")\n",
    "    vectors_np = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Busca Vetorial e Filtragem:** Calcula a matriz de similaridade entre todos os conceitos e aplica regras r√≠gidas (threshold > 0.89, top-k=5) para selecionar pares candidatos a conex√£o, evitando a cria√ß√£o de 'super-hubs' ou conex√µes fracas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando Matriz de Similaridade...\n",
      "Candidatos Matem√°ticos Encontrados: 16\n",
      "   (Crit√©rios: Sim > 0.82, Top-8, Sem conex√µes pr√©vias)\n"
     ]
    }
   ],
   "source": [
    "## 4. Algoritmo de Densifica√ß√£o (Matem√°tica)\n",
    "\n",
    "# --- AJUSTE ESTRAT√âGICO: Diminui√ß√£o do Threshold ---\n",
    "# Antes: 0.89 (Muito estrito) -> Agora: 0.82 (Permissivo, o Agente Juiz da Fase 4 limpar√°)\n",
    "SIMILARITY_THRESHOLD = 0.82 \n",
    "TOP_K = 8 # Aumentado de 5 para 8 para capturar mais vizinhos potenciais\n",
    "\n",
    "candidates = [] # Lista de tuplas (idx_A, idx_B, score)\n",
    "\n",
    "if vectors_np is not None:\n",
    "    print(\"\\nCalculando Matriz de Similaridade...\")\n",
    "    sim_matrix = cosine_similarity(vectors_np)\n",
    "    \n",
    "    count_candidates = 0\n",
    "    \n",
    "    # Iterar sobre cada n√≥ para encontrar vizinhos\n",
    "    for i in range(len(clean_concepts)):\n",
    "        # Pegar scores para o n√≥ i\n",
    "        scores = sim_matrix[i]\n",
    "        \n",
    "        # Zerar o score dele mesmo para n√£o ser selecionado\n",
    "        scores[i] = 0\n",
    "        \n",
    "        # Filtrar por Threshold\n",
    "        # Retorna √≠ndices onde score > threshold\n",
    "        high_sim_indices = np.where(scores > SIMILARITY_THRESHOLD)[0]\n",
    "        \n",
    "        # Ordenar por score decrescente e pegar Top-K\n",
    "        # (Precisamos ordenar os √≠ndices filtrados baseados em seus scores)\n",
    "        top_indices = sorted(high_sim_indices, key=lambda idx: scores[idx], reverse=True)[:TOP_K]\n",
    "        \n",
    "        source_name = clean_concepts[i]['nome']\n",
    "        \n",
    "        for j in top_indices:\n",
    "            target_name = clean_concepts[j]['nome']\n",
    "            \n",
    "            # Verificar se j√° existe conex√£o (Dire√ß√£o A->B ou B->A)\n",
    "            # Queremos evitar redund√¢ncia se o grafo for n√£o-direcionado semanticamente,\n",
    "            # mas aqui √© direcionado. Por√©m, se j√° existe A->B, n√£o sugerimos de novo.\n",
    "            if not G_initial.has_edge(source_name, target_name):\n",
    "                candidates.append({\n",
    "                    \"source\": clean_concepts[i],\n",
    "                    \"target\": clean_concepts[j],\n",
    "                    \"score\": float(scores[j])\n",
    "                })\n",
    "                count_candidates += 1\n",
    "\n",
    "    print(f\"Candidatos Matem√°ticos Encontrados: {len(candidates)}\")\n",
    "    print(f\"   (Crit√©rios: Sim > {SIMILARITY_THRESHOLD}, Top-{TOP_K}, Sem conex√µes pr√©vias)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valida√ß√£o Sem√¢ntica (LLM):** Submete os candidatos encontrados matematicamente √† an√°lise do GPT-4o-mini. O modelo decide se a rela√ß√£o faz sentido tecnicamente e atribui o tipo correto (USE, RELATED_TO) ou descarta o par (SKIP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Arquiteto Validando Candidatos...\n",
      "   Batch 1: 8 aprovados de 16 enviados.\n",
      "Total de novas conex√µes validadas: 8\n"
     ]
    }
   ],
   "source": [
    "## 5. Agente 2: The Architect (Valida√ß√£o LLM)\n",
    "\n",
    "class ValidatedEdge(BaseModel):\n",
    "    source: str\n",
    "    target: str\n",
    "    relation_type: Literal['RELATED_TO', 'USE', 'SKIP']\n",
    "    explanation: str\n",
    "\n",
    "class ValidationOutput(BaseModel):\n",
    "    edges: List[ValidatedEdge]\n",
    "\n",
    "architect_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Voc√™ √© um Arquiteto de Ontologias Linux.\n",
    "        Receba pares de conceitos com alta similaridade vetorial.\n",
    "        Sua tarefa: Validar se existe rela√ß√£o l√≥gica e tipific√°-la.\n",
    "\n",
    "        REGRAS:\n",
    "        1. Se forem Sin√¥nimos ou Variantes (ex: 'ls' e 'listar'): Use 'RELATED_TO'.\n",
    "        2. Se um √© ferramenta/comando do outro (ex: 'apt' e 'Gerenciador de Pacotes'): Use 'USE'.\n",
    "        3. Se for apenas coincid√™ncia de palavras sem rela√ß√£o t√©cnica direta: Use 'SKIP'.\n",
    "        \"\"\"),\n",
    "        (\"user\", \"\"\"Analise estes candidatos:\n",
    "        {candidates}\n",
    "        \n",
    "        {format_instructions}\"\"\")\n",
    "    ])\n",
    "    | llm_mini\n",
    "    | PydanticOutputParser(pydantic_object=ValidationOutput)\n",
    ")\n",
    "\n",
    "validated_edges = []\n",
    "\n",
    "if candidates:\n",
    "    print(\"\\n Arquiteto Validando Candidatos...\")\n",
    "    \n",
    "    # Batching para LLM\n",
    "    BATCH_SIZE = 20\n",
    "    \n",
    "    for i in range(0, len(candidates), BATCH_SIZE):\n",
    "        batch = candidates[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Formatar input compacta para economizar tokens\n",
    "        batch_input = []\n",
    "        for item in batch:\n",
    "            s = item['source']\n",
    "            t = item['target']\n",
    "            batch_input.append(f\"- {s['nome']} ({s['tipo']}) <--> {t['nome']} ({t['tipo']}) [Sim: {item['score']:.2f}]\")\n",
    "            \n",
    "        try:\n",
    "            res = architect_chain.invoke({\n",
    "                \"candidates\": \"\\n\".join(batch_input),\n",
    "                \"format_instructions\": PydanticOutputParser(pydantic_object=ValidationOutput).get_format_instructions()\n",
    "            })\n",
    "            \n",
    "            valid_ones = [e for e in res.edges if e.relation_type != 'SKIP']\n",
    "            validated_edges.extend(valid_ones)\n",
    "            print(f\"   Batch {i//BATCH_SIZE + 1}: {len(valid_ones)} aprovados de {len(batch)} enviados.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Erro no Batch {i}: {e}\")\n",
    "\n",
    "    print(f\"Total de novas conex√µes validadas: {len(validated_edges)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agente Pedagogo (Teacher):** Analisa as novas conex√µes (e as existentes) para identificar depend√™ncias de aprendizado cr√≠ticas, promovendo rela√ß√µes para 'PREREQUISITE' quando um conceito √© fundamental para entender o outro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéì Teacher analisando novas rela√ß√µes...\n",
      "Teacher promoveu 2 rela√ß√µes para PREREQUISITE.\n"
     ]
    }
   ],
   "source": [
    "## 6. Agente 3: The Teacher (Pedagogo)\n",
    "# Converte rela√ß√µes funcionais (USE/RELATED) em Pedag√≥gicas (PREREQUISITE)\n",
    "\n",
    "class PrerequisiteCheck(BaseModel):\n",
    "    indices: List[int] = Field(description=\"Indices das rela√ß√µes que s√£o Pr√©-requisitos\")\n",
    "\n",
    "teacher_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Identify LEARNING DEPENDENCIES. If knowing A is mandatory to understand B, select it.\"),\n",
    "        (\"user\", \"Relations:\\n{items}\\n{format_instructions}\")\n",
    "    ])\n",
    "    | llm_mini\n",
    "    | PydanticOutputParser(pydantic_object=PrerequisiteCheck)\n",
    ")\n",
    "\n",
    "# Converter validated_edges para formato dict compat√≠vel\n",
    "new_relations_dicts = [\n",
    "    {\"source\": e.source, \"target\": e.target, \"type\": e.relation_type, \"explanation\": e.explanation} \n",
    "    for e in validated_edges\n",
    "]\n",
    "\n",
    "# Combinar com antigas (que n√£o eram prereq)\n",
    "candidates_teacher = new_relations_dicts\n",
    "\n",
    "if candidates_teacher:\n",
    "    print(\"\\nüéì Teacher analisando novas rela√ß√µes...\")\n",
    "    BATCH_SIZE = 50\n",
    "    upgraded_count = 0\n",
    "    \n",
    "    for i in range(0, len(candidates_teacher), BATCH_SIZE):\n",
    "        batch = candidates_teacher[i:i+BATCH_SIZE]\n",
    "        batch_fmt = [f\"{idx}: {r['source']} -> {r['target']} ({r['type']})\" for idx, r in enumerate(batch)]\n",
    "        \n",
    "        try:\n",
    "            res = teacher_chain.invoke({\n",
    "                \"items\": \"\\n\".join(batch_fmt),\n",
    "                \"format_instructions\": PydanticOutputParser(pydantic_object=PrerequisiteCheck).get_format_instructions()\n",
    "            })\n",
    "            \n",
    "            for rel_idx in res.indices:\n",
    "                if rel_idx < len(batch):\n",
    "                    real_idx = i + rel_idx\n",
    "                    candidates_teacher[real_idx]['type'] = 'PREREQUISITE'\n",
    "                    candidates_teacher[real_idx]['explanation'] += \" [Teacher: PREREQ]\"\n",
    "                    upgraded_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Erro Teacher: {e}\")\n",
    "            \n",
    "    print(f\"Teacher promoveu {upgraded_count} rela√ß√µes para PREREQUISITE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consolida√ß√£o Final:** Unifica as rela√ß√µes originais com as novas conex√µes densificadas, recalcula as m√©tricas do grafo (densidade) e salva o resultado final para uso no SINKT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "RELAT√ìRIO DE DENSIFICA√á√ÉO AGRESSIVA\n",
      "========================================\n",
      "Densidade: 0.00585 -> 0.00435\n",
      "N√≥s: 230 -> 204\n",
      "Arestas: 211 -> 180 (+8)\n",
      "üíæ Grafo Denso Salvo: output/02_densification/enhanced_graph.json\n"
     ]
    }
   ],
   "source": [
    "## 7. Consolida√ß√£o e Relat√≥rio\n",
    "\n",
    "# Unir Antigas + Novas\n",
    "final_relations = relations + candidates_teacher\n",
    "\n",
    "# Construir Grafo Final\n",
    "G_final = nx.DiGraph()\n",
    "for c in clean_concepts:\n",
    "    G_final.add_node(c['nome'])\n",
    "\n",
    "for r in final_relations:\n",
    "    if G_final.has_node(r['source']) and G_final.has_node(r['target']):\n",
    "        G_final.add_edge(r['source'], r['target'], type=r['type'])\n",
    "\n",
    "old_density = nx.density(G_initial)\n",
    "new_density = nx.density(G_final)\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"RELAT√ìRIO DE DENSIFICA√á√ÉO AGRESSIVA\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Densidade: {old_density:.5f} -> {new_density:.5f}\")\n",
    "print(f\"N√≥s: {len(concepts)} -> {G_final.number_of_nodes()}\")\n",
    "print(f\"Arestas: {len(relations)} -> {G_final.number_of_edges()} (+{len(candidates_teacher)})\")\n",
    "\n",
    "output_data = {\n",
    "    \"concepts\": clean_concepts,\n",
    "    \"relations\": final_relations,\n",
    "    \"metrics\": {\n",
    "        \"density\": new_density,\n",
    "        \"edges_added\": len(candidates_teacher)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(ENHANCED_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "print(f\"üíæ Grafo Denso Salvo: {ENHANCED_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
